
\section{Time Series Basics}

\textbf{Stochastic process (SP)}: sequence of random variables indexed in time order: $ Y_1, Y_2, \ldots, Y_t, \ldots $

Realization of an SP yields a time series: $ \hat{Y}_1=y_1, \hat{Y}_2=y_2, \ldots, \hat{Y}_t=y_t, \ldots $

\begin{minipage}[t]{0.55\textwidth}
\textbf{Statistical forecasting notation:}\\
\begin{tabular}{ll}
    Past observations & $\I = \{\hat{Y}_1=y_1, \hat{Y}_2=y_2, \ldots, \hat{Y}_{t-1}=y_{t-1}\}$ \\
    Conditioning & $Y_t | \I$ \\
    Point forecast & \(\mean(Y_t | \I)\) \\
    Forecast variance & \(\var(Y_t | \I)\) \\
    Conditioning on time series & $Y_{t|t-1} = Y_t | \{y_1, y_2, \ldots, y_{t-1}\}$ \\
    \(h\)-step forecast & \(Y_{T+h|t-1} = \E[Y_{T+h} | \{y_1, y_2, \ldots, y_{t-1}\}]\) \\
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.40\textwidth}
\textbf{Time series expressions:}\\
\begin{tabular}{ll}
    Trend & long-term increase/decrease \\
    Seasonal & predictable, seasonal changes \\
    Cyclic & changes that are not of a fixed period \\
\end{tabular}
\end{minipage}

\begin{minipage}[t]{0.5\textwidth}
\textbf{Autocorrelation function} (aka Autocorrelogram):\\
\begin{tabular}{ll}
    Seasonality & High positive values at multiples of seasonality \\
    Trend & Higher absolute values for small lags \\
    White Noise & All values below critical threshold of \(\pm 1.96 / \sqrt{T}\) \\
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.46\textwidth}
\textbf{Statistical Hypothesis Testing (SHT)}:
\begin{itemize}
    \item Test time series against null hypothesis (e.g.\ white noise)
    \item Statistical test (e.g.\ Ljung-Box) yields p-Value
    \item Very small p-Value \(\Leftrightarrow\) reject null hypothesis
\end{itemize}
\end{minipage}

\begin{minipage}[t]{0.63\textwidth}
\textbf{Residuals}: difference between observed value and its fitted value: $ e_t = y_t - \hat{y}_{t|t-1} $
\begin{itemize}
    \item Residuals have non-zero mean \(\Leftrightarrow\) model is biased.
    \item Residuals are correlated \(\Leftrightarrow\) they contain information not captured by the model.
    \item Assumption: Residuals have constant variance and are normally distributed.
\end{itemize}
\(\Rightarrow\)Residuals should be indistinguishable from white noise \(\rightarrow\) test using SHT
(small p-value \(\Leftrightarrow\) residuals are correlated).\@

\vspace{2mm}
\textbf{Cross-validation} for time series: test set comes after its training set (in time)

\vspace{2mm}
\textbf{Prediction intervals}: Confidence region the forecast value is expected to be in.
The 95\% prediction interval for an $ h $-step forecast can be computed
as $ \hat{y}_{T+h|T} \pm 1.96 \hat{\sigma}_h $, with $ \hat{\sigma}_h $ as the stddev of the $ h $-step distribution.
Assuming the \emph{residuals are normally distributed},
$ \hat{\sigma}_h $ can be estimated for different forecast methods by the formulas below.

\vspace{8mm}

The \emph{error metrics} $ \MAE $, $ \MSE $, $ \RMSE $ and $ \SSE $ are all scale dependent.
$ \MAPE $ is scale independent but is only sensible if $ y_t \gg 0 \text{ for all } t $, and $ y $ has a natural zero.
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\textwidth}
\textbf{Error metrics} (for \(n\) values):
\begin{align*}
    \MAE &= \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \\
    \MSE &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \\
    \MAPE &= \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \cdot 100 \\
    \RMSE &= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \\
    \MASE &= \frac{1}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{\frac{1}{n-1} \sum_{i=2}^{n} |y_i - y_{i-1}|} \\
    \SSE &= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{align*}
\end{minipage}

\begin{minipage}[t]{0.63\textwidth}
\section{Forecasting}
\begin{itemize}
\item forecast $ \hat{y}_{t|t-1} $ $\Leftrightarrow$ fitted value of $ y $ at time $ t $ given past values up to time $ t - 1 $
\item We can perform $h$ step forecasts by iteratively applying one step forecasts.
\item There are different models to perform forecasts.
\end{itemize}
\end{minipage}

\subsection{Simple Forecast Methods}
\begin{tabular}{@{}llll@{}}\toprule
    Method & Description & Formula for \(h\) step forecast & Confidence \(\hat{\sigma}_h\)\\ \midrule
    Average & Mean of all past observations & \(\hat{y}_{T+h|T} = \bar{y} = (y_1 + \cdots + y_T) / T \) &  \(\hat{\sigma} \cdot \sqrt{1+1/T}\) \\
    Naïve & Value of last observation & \(\hat{y}_{T+h|T} = y_T\) & \(\hat{\sigma} \cdot \sqrt{h}\) \\
    Seasonal Naïve & Last observed value from the same season & \(\hat{y}_{T+h|T} = y_{T+h-m(k+1)}\) & \(\hat{\sigma} \cdot \sqrt{k+1}\)\\
    Drift & Last value plus average historical change & \(\hat{y}_{T+h|T} = y_T + \frac{h}{T-1}(y_T-y_1)\) & \(\hat{\sigma} \cdot \sqrt{h \cdot (1+h/T)}\)\\
    \bottomrule
\end{tabular}

In the table above, \(T\) is the number of historical values, \(\hat{\sigma}\) is the standard deviation of the residuals,
\(m\) is the seasonal period (e.g.\ 12 for monthly data), and \(k = \floor[(h-1)/m]\) is the number of whole seasons.


\subsection{Exponential Smoothing}
\(\rightarrow\) Weighted average of historical data with exponentially decreasing weights.
Parameters can be found by minimizing the $ \SSE $.

\begin{minipage}[t]{0.49\textwidth}
\textbf{Simple Exponential Smoothing (SES)} \((N,N)\): no trend or cyclic components \(\rightarrow\) flat forecasts for all time horizons\\
$ \hat{y}_{T+1|T} = \alpha y_{T} + \alpha (1 - \alpha) y_{T-1} + \alpha (1 - \alpha)^2 y_{T-2} + \ldots $,
where $ 0 \le \alpha \le 1 $. The weights sum up to one as a geometric series.\\
Forecast Equation: $ \hat{y}_{t+h|t} = \ell_t $\\
Smoothing Equation: $ \ell_t = \alpha y_t + (1 - \alpha) \ell_{t-1} $
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\textbf{Holt's linear trend} \((A,N)\)\\
Forecast Equation: $ \hat{y}_{t+h|t} = \ell_t + h b_{t} $\\
Smoothing Equation: $ \ell_t = \alpha y_t + (1 - \alpha) (\ell_{t-1} + b_{t-1}) $\\
Trend Equation: $ b_t = \beta^* (\ell_t - \ell_{t-1}) + (1 - \beta^*) b_{t-1} $

\textbf{Holt's damped trend} \(A_d,N\)\\
Forecast Equation: $ \hat{y}_{t+h|t} = \ell_t + (\phi + \phi^2 + \cdots + \phi^h) b_{t} $\\
Smoothing Equation: $ \ell_t = \alpha y_t + (1 - \alpha) (\ell_{t-1} + \phi b_{t-1}) $\\
Trend Equation: $ b_t = \beta^* (\ell_t - \ell_{t-1}) + (1 - \beta^*) \phi b_{t-1} $
\end{minipage}

\begin{minipage}[t]{0.49\textwidth}
\textbf{Holt-Winters additive seasonality (A,A)}\\
Forecast Equation: $ \hat{y}_{t+h|t} = \ell_t + hb_{t} + s_{t+h-m(k+1)} $\\
Smoothing Equation: $ \ell_t = \alpha (y_t - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1}) $\\
Trend Equation: $ b_t = \beta^* (\ell_t - \ell_{t-1}) + (1 - \beta^*) b_{t-1} $\\
Seasonality Equation: $ s_t = \gamma (y_t - \ell_{t-1} - b_{t-1}) + (1 - \gamma) s_{t-m} $
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\textbf{Holt-Winters multiplicative seasonality (A,M)}\\
Forecast Equation: $ \hat{y}_{t+h|t} = (\ell_t + hb_{t-1}) \cdot s_{t-m+h-m(q+1)} $\\
Smoothing Equation: $ \ell_t = \alpha \frac{y_t}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1}) $\\
Trend Equation: $ b_t = \beta (\ell_t - \ell_{t-1}) + (1 - \beta) b_{t-1} $\\
Seasonality Equation: $ s_t = \gamma \frac{y_t}{\ell_{t-1} + b_{t-1}} + (1 - \gamma) s_{t-m} $
\end{minipage}

\begin{minipage}[t]{0.49\textwidth}
\textbf{Exponential Smoothing State Space Models (ETS)}\\
Contrary to pure forecasting methods, State Space Models can provide point forecasts as well as prediction intervals.

ETS models have three parameters:\\
\textbf{E}rror \(\in \{A, M\}\) for \{additive, multiplicative\}\\
\textbf{T}rend \(\in \{N, A, A_d\}\) for \{none, additive, additive damped\}\\
\textbf{S}easonal \(\in \{N, A, M\}\) for \{none, additive, multiplicative\}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\textbf{Parameter ranges}:
\(0 \le \alpha, \beta^*, \phi \le 1, \quad 0 \le \gamma \le  1-\alpha\)

\vspace{2mm}
\textbf{Automatic Forecasting} aims to find the best performing model out of all possible permutations above by choosing
the model with the lowest $ AIC = -2 \ln(\mathcal{L}) + 2k $ or $ AICc = AIC + \frac{2k(k+1)}{n-k-1} $,
where $ \mathcal{L} $ is the likelihood of the model, $ k $ is the number of estimated parameters in the model and $ n $ is the number of observations.
\end{minipage}


\subsection{Autoregressive Integrated Moving Average (ARIMA)}
Combines autoregressive (AR) and moving average (MA) components along with differencing (I).

\begin{minipage}[t]{0.49\textwidth}
\textbf{Random Walk (with drift)}: $ y_t = c + y_{t-1} + \epsilon_t $

\textbf{Stationarity}: Statistical properties remain constant over time.
The ACF of stationary processes quickly drops to zero and there is no seasonality visible.
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\textbf{Differencing} can be used (repeatedly) to obtain a stationary time series.\\
\textbf{Backshift notation}: $ By_t = y_{t-1}$; $y_t' = y_t - y_{t-1} = (1 - B) \cdot y_t$; $y_t'' = (1 - B)^2 \cdot y_t$;
Seasonal: $y'_t = y_t - y_{t-m} = (1-B^m)y_t $
\end{minipage}

\begin{minipage}[t]{0.49\textwidth}
\textbf{Autoregressive (AR) Models} use multiple regression with lagged values of $ y_t $ as predictors:
$ y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t $, denoted as \(\AR(p)\).

They work only on stationary data. General condition for stationarity:
Complex roots of \(1 - \phi_1z - \phi_2z^2 - \cdots - \phi_p z^p\) lie outside the unit circle.
For \(q=1\): \(|\phi_1| < 1\).

\vspace{2mm}
\textbf{Moving Average (MA) Models} use multiple regression with past noise as predictors:
$ y_t = c + \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \cdots - \theta_q \varepsilon_{t-q} $, denoted as \(\MA(q)\).

Any $ \MA(q) $ process can be written as an $ \AR(\infty) $ when the roots of the characteristic polynomial lie outside the unit circle.
General condition for invertibility:
Complex roots of \(1 + \theta_1z + \theta_2z^2 + \cdots + \theta_q z^q\) lie outside the unit circle.
For \(q=1\): \(|\theta_1| < 1\).
For \(q=2\): \(|\theta_2| < 1,\quad \theta_1 + \theta_2 > -1,\quad \theta_1-\theta_2 < 1\)
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\textbf{ARMA}: \(y_t = c + \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} +
\theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q}\)

\textbf{ARIMA}\((p,d,q)\): Combine ARMA with differencing\\
p = order of the AR part\\
d = degree of first differencing involved\\
q = order of the MA part\\
ARIMA = ARMA followed by \((1-B)^d y_t\)\\
ARIMA\((1,1,1)\) model:
\(\underbrace{(1-\phi_1B)}_{\AR(1)} \underbrace{(1-B)}_{\text{1st diff}}y_t = c + \underbrace{(1 + \theta_1B)\varepsilon_t}_{\MA(1)}\)\\
ARIMA\((1,1,1)\): \( y_t = c + y_{t-1} + \phi_1(y_{t-1} - y_{t-2}) + \theta_1 \varepsilon_{t-1} + \varepsilon_t\)

\vspace{2mm}
ARIMA model examples:\\
\begin{tabular}{ll}
    ARIMA\((0, 0, 0)\) & white noise model\\
    ARIMA\((0, 1, 0), c=0\) & random walk\\
    ARIMA\((0, 1, 0), c\ne0\) & random walk with drift\\
    ARIMA\((p, 0, 0)\) & \(\AR(p)\)\\
    ARIMA\((0, 0, q)\) & \(\MA(q)\)
\end{tabular}
\end{minipage}
