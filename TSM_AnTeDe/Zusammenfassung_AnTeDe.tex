\documentclass[11pt]{article}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage[a4paper,margin=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{isodate}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{tabularx}
\usepackage{ltablex} % Longtables with tabularx
\usepackage[x11names]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{scalerel}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage[linguistics]{forest}
\usepackage{pdfpages}
\usepackage{csquotes}

% Bibliography & citing
\usepackage[
	backend=biber,
	style=apa,
	bibstyle=apa,
	citestyle=apa,
	sortlocale=en_UK
]{biblatex}
\addbibresource{Bibliography.bib}
% print the whole bibliography
\nocite{*}

% Code highlighting
\usepackage{minted}
\surroundwithmdframed{minted}

% Be able to caption equations and float them in place
\usepackage{float}

\newmdtheoremenv{theorem}{Theorem}
\geometry{a4paper, margin=2.4cm}
\newtheorem*{remark}{Remark}

\newcommand\equalhat{\mathrel{\stackon[1.5pt]{=}{\stretchto{\scalerel*[\widthof{=}]{\wedge}{\rule{1ex}{3ex}}}{0.5ex}}}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\newcommand{\stack}{{\color{SlateGray4} stack}\ }
\newcommand{\buffer}{{\color{Orange1} buffer}\ }
\newcommand{\dependencygraph}{{\color{DodgerBlue3} dependency graph}\ }

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\graphicspath{{./img/}}

\begin{document}
	
\title{Analysis of Text Data FS20}
\author{Pascal Baumann\\pascal.baumann@stud.hslu.ch}
\maketitle



For errors or improvement raise an issue or make a pull request on the \href{https://github.com/KilnOfTheSecondFlame/mse_summaries}{github repository}.

\tableofcontents
\newpage

\section{Introduction}
Text analysis consist of a series of operations completed by one or more pieces of software on a sample of written human language, with the goal of extracting useful information.

Applications of text analysis include:
\begin{itemize}[noitemsep]
	\item Analysis
	\begin{itemize}
		\item spell checkers
		\item keyword extraction
		\item authorship attribution
		\item document retrieval
		\item text classification
		\item text mining
		\item sentiment analysis
		\item content-based recommendation
	\end{itemize}
	\item Text Analysis and Generation
	\begin{itemize}
		\item machine translation
		\item automatic question answering
		\item automatic summarisation
	\end{itemize}
	\item Text Generation
	\begin{itemize}
		\item database report generation
		\item weather forecast generation
	\end{itemize}
	\item Analysis, Generation and Interaction
	\begin{itemize}
		\item dialogue systems
		\item assistive technology for teaching or writing
	\end{itemize}
\end{itemize}

Analysis of Text Data lies at the intersection of Machine Learning, Computational Linguistics and Human-Computer Interaction. Artificial Intelligence is applied to human language, while human speech data is a related problem which makes use of different technologies, like signal processing.

\subsection{Basic Concepts in NLP}
Machine Learning is a powerful tool in NLP, thus the same vocabulary is used. Most of the time, supervised learning is used for Text Analysis, where the labelled data is classified by human experts and called the reference or ground truth.

Significance addresses the key question if the difference between the two systems is really due to the fact that one is better than the other or if it can be explained by randomness. This is easier to compute when cross-validation is used, and paired $t$-tests can be used to compare two systems. Performance scores vary depending on the data set it is difficult to predict actual performance on a data set of a different
nature than the test set, this is called the problem of portability.

\subsection{Key Jargon from Linguistics}
\begin{itemize}
	\item letter
	\item syllable
	\item morpheme: a meaningful morphological unit of a language that cannot be further divided
	\item word
	\item phrase: a group of words standing together as a conceptual unit
	\item clause: a group of words in a sentence that contains a subject and a subject
	\item sentence:
	\begin{itemize}
		\item simple sentence: one independent clause
		\item compound sentence: at least two independent clauses
		\item complex sentence: at least one independent clause and one or more dependent clauses
	\end{itemize}
	\item text
	\item corpus: collection of text, pl. corpora
	\item utterance: an uninterrupted chain of spoken or written language
	\item lexeme: basic abstract unit of meaning that roughly corresponds to a set of forms taken by a single root word — for example run, runs, ran and running are forms of the same lexeme
	\item lemma: one form chosen by convention as the canonical form of a lexeme — go, goes, went, gone, going for the lexeme \emph{go}
\end{itemize}

\subsection{Utterance Decoding}
Decode its propositional content or logical form first and then combine them. Draw inference to solve ambiguities, guess what is left unsaid and, in general, maximise the likelihood given the context.

\subsection{Lexical Analysis}
The goal of lexical analysis is to understand word forms.
\begin{enumerate}
	\item tokenisation: breaking a text into word tokens
	\item lemmatisation: finding the base form of each word token or lemma
	\item Part of Speech tagging: finding the part of speech each word token corresponds to
	\item Named Entity Recognition:  identification of proper nouns of people, places, organizations
\end{enumerate}

\subsection{Syntactic Analysis}
\begin{itemize}
	\item sentence segmentation or splitting
	\item identifying phrases or chunking
	\item figuring out logical functions
	\item building parse trees or parsing
\end{itemize}

\subsubsection{Constituency Parsing}
With syntactic parsing the connection between words can be understood. Constituency parsing breaks a sentence into its basic building blocks.

\begin{center}
	\includegraphics[width=\linewidth]{img/parse_tree_syntactic_parsing}
\end{center}

\subsubsection{Dependency Parsing}
Dependency Parsing helps in understanding what depends on what. The assumption is that the sentence is centred around the verb and what comes first in a sentence is more important than any information that comes after. So anything of the sentence depends on that word or some lower-level information.

\begin{center}
	\includegraphics[width=\linewidth]{img/parse_tree_dependency_parsing}
\end{center}

\subsection{Understanding Meaning}
\subsubsection{Semantic Analysis}
\begin{itemize}
	\item word-level
	\begin{itemize}
		\item word sense disambiguation
		\item co-occurrence analysis
	\end{itemize}
	\item sentence-level or text-level
	\begin{itemize}
		\item semantic role labelling
		\item co-reference resolution
	\end{itemize}
\end{itemize}

\subsubsection{Discourse Analysis}
\begin{itemize}
	\item topics
	\item sentiments
	\item speech or dialogue acts
	\item argumentative structures
\end{itemize}

\subsection{Data for Text Analysis}
Text analysis using machine learning requires large amounts of training data and finding suitable data is often a bottleneck due to expense or limited rights. An annotated corpus typically contains a selection of texts based on explicit criteria, metadata like author, date, source, title, sectioning, and annotations in a more or less standardised format.

\subsection{Preprocessing}
Most text processing tasks begin with a set of standard preprocessing steps
\begin{itemize}
	\item Tokenisation: segmentation into tokens
	\item Token normalisation
	\item Segmentation into sentences
	\begin{center}
		\includegraphics[width=0.8\linewidth]{img/sentence_segmentation}
	\end{center}
	\item There are about 170'000 unique words in the English language at the moment
\end{itemize}

\subsubsection{Tokens and Types}
Tokens are the words on the page, while the type is the word forms. Counting the types requires lemmatisation, which is finding the lemma or base form for each word.

\subsubsection{Tokenisation and Normalisation}
Not as straightforward as one may think
\begin{itemize}
	\item punctuation
	\begin{itemize}
		\item periods and commas appear within words or abbreviations
		\item special tokens in mail addresses or tweets
		\item apostrophes
	\end{itemize}
	\item capital letters
	\item compound words
	\begin{itemize}
		\item problem complicated without dashes
		\item German words
		\item proper names
	\end{itemize}
\end{itemize}

\subsubsection{Lemmatisation and Stemming}
Stemming is the process of reducing inflected or derived words to their word stem, also called base or root form. Lemmatisation refers to the process of determining the lemma of a word based on its intended meaning. Lemmatisation is closely related to stemming, but a stemmer \emph{operates without knowledge of the context of the word}. Lemmatisation, on the other hand, depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as the larger context surrounding that sentence.

\subsubsection{Sentence Segmentation}
Can be easier or more difficult depending on the source text formatting. If no particular information is available from the layout, punctuations and casing can be used. While question and exclamation marks are quite reliable indicator of sentences periods are not. A good approach is to try to combine Tokenisation and Sentence Splitting, focusing on full stops.

\section{Text Classification}
The goal of text classification is to assign text documents to one or more categories. There is a predefined set of classes, in which previously unseen documents are assigned to. If there are only two classes this is a binary classification problem.

To tackle this problem there are different strategies: (a) have hard-coded rules carefully crafted by an expert on the basis on combinations of words or other features, (b) through supervised machine learning with understanding building on semantic representation of texts and labels or (c) supervised machine learning without understanding where word-based features are derived from text and the relationship between features and labels from the training texts.

Stop-words are words like conjunctions or prepositions, which may not have meaning given the task at hand.

\subsection{Formalising Text Classification}

\begin{itemize}[label=]
	\item Input
	\begin{itemize}
		\item a set of documents $D$
		\item a fixed set $N$ of classes $C$
		\item a training set of $M$ hand-labelled documents $(d_1 C_1),(d_2 C_2), \dots (d_M C_M)$
	\end{itemize}
	\item Output
	\begin{itemize}
		\item a mapping $D\rightarrow C$ that associates a predicted class $c\in C$ to each document $d\in D$
	\end{itemize}
\end{itemize}

\subsection{Naïve Bayes Classifier}
For a document $d$ and a class $c$

\begin{equation*}
	P(c|d) = \frac{P(d|c)P(c)}{P(d)}
\end{equation*}

\noindent
Maximum A Posteriori (MAP) classifier:
\begin{equation*}
	c_{\text{MAP}} = \underset{c\in C}{\text{argmax}} \frac{P(d|c) P(c)}{P(d)} = \underset{c\in C}{\text{argmax}} P(d|c) P(c)
\end{equation*}

Dropping the denominator does not change $c_{\text{MAP}}$ because $P(d)$ has no effect on argmax. In practice, the evidence a machine can observe is not the human-readable document $d$, but a number of features $x_1,\dots,x_N$ obtained based on $d$ and a MAP-classifier
\begin{equation*}
	c_MAP = \underset{c\in C}{\text{argmax}} P(x_1, x_2, \dots , x_N | c) P(c)
\end{equation*}

\noindent
Given a vocabulary of V words a feature can be if a word appears in a document $d$ or how often it appears
\begin{itemize}
	\item \textbf{Bernoulli Model} represent $d$ as $(e_1, \dots, e_V)$ where $e_i =1$ if the word $i$ is in $d$ and $e_i = 0$ otherwise
	\item \textbf{Multinomial Model} represent $d$ as $f_1, \dots, f_V)$ where $f_i$ is the number of occurrences of word $i$ in $d$
\end{itemize}

The Naïve Bayes independence assumption is that given a class $c$ the \textbf{features are independent}
\begin{equation*}
	P(x_1, x_2, \dots , x_N | c) = P(x_1|c)\cdot P(x_2|c) \cdots P(x_N|c) = \prod_{k=1}^{n}P(x_k|c)
\end{equation*}

$P(c)$ and $P(x_k|c)$ need to be computed, $P(c)$ can be estimated based on the frequency of each class in the training data, and $P(x_k|c)$ depends on the chosen feature representation.

In the so called Bag-Of-Word Model $x_k$ is the feature representation of the words in the documents. The position of the individual words can usually be ignored.

\subsection{Multinomial Naïve Bayes}
Represent every token in $d$ as a feature vector $x_i = f_i$ , where $f_i$ is the number of occurrences of token $i$ in $d$. Then $P(x_i|c)$ can be estimated as $\frac{\text{number of occurrences of token }i\text{ in }c}{\text{total number of tokens in }c}$. For simplicity $P(w|c)$ can be written to refer to the probability of finding token $w$ in class $c$.

\begin{enumerate}
	\item Normalise the training data (remove stop words, remove punctuation, set all characters to lowercase)
	\item Assemble vocabulary (list of unique meaningful words)
	\item Count the number of occurrences of each word in each class and divide by the total number of words in each class
\end{enumerate}

\subsection{Practical Complications}
If a word $w$ from the vocabulary (which contains $V$ tokens) is never in class $c$ in the training data, we will estimate $P(w|c) = 0$, which causes $c_{\text{corpus}}=\underset{c\in C}{\text{argmax}}P(c)\prod_{k=1}^{N}P(x_k|c)=0$.

\begin{remark}
	Zero probabilities cannot be conditioned away, no matter the other evidence!\\ - Dan Jurafsky
\end{remark}

The typical solution to this is to do Laplace Smoothing like this
\begin{equation*}
	P(w|c)\text{ can be estimated as }\frac{\text{number of occurrences of token $w$ in class } c + 1}{\text{total number of tokens in class } c+V}
\end{equation*}
This prevents probabilities of zero from occurring.

\subsection{Metrics}
\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=1cm]
	\item[True Positive] Sample $c$ classified correctly
	\item[False Positive] Non-$c$ sample incorrectly classified as $c$
	\item[True Negative] Non-$c$ sample classified correctly
	\item[False Negative] Sample $c$ classified incorrectly as non-$c$
\end{itemize}

\begingroup
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\linewidth}{rX}
		Accuracy & $ \frac{\sum TP}{\sum \text{all elements}} $ \\
		Precision & $\frac{TP}{TP + FP}$\\
		Recall & $ \frac{TP}{TP + FN} $\\
	\end{tabularx}
\endgroup

\section{Sentiment Analysis}
Sentiment analysis tries to understand the emotional state of the author of a text, and is also called opinion mining but can be extremely hard. In principle text classification and sentiment analysis are very different, but there are some striking similarities as the text can reflect a positive or a negative emotional state which can be utilised.

\subsection{Framing Sentiment Analysis}
Sentiment analysis is commonly framed as \textbf{attitude detection}
\begin{itemize}
	\item \emph{Attitude} is an enduring disposition toward someone or something with an emotional connotation
	\item The attitude can be broken down in quintuplets
	\begin{enumerate}
		\item opinion holder
		\item target entity
		\item aspect: specific feature of the target that the opinion is about
		\item type: most commonly {\color{Green3} positive}, {\color{Firebrick3} negative}, {\color{gray} neutral} and with an indication of strength
		\item time when this opinion was expressed
	\end{enumerate}
	\item knowing the opinion without knowing the opinion target is of limited use
\end{itemize}

\subsection{Tokenisation Challenges}
\begin{itemize}
	\item Isolating emoticons
	\item Respecting domain-specific markup like \mintinline{html}{<strong>really bad idea</strong>}
	\item Capturing masked curses
	\item Selective true-casing
	\item Normalised lengthening, as "YAAAAAAY" is equivalent to "YAY" no matter how it is written
	\item Capturing important multi-word expressions and idioms
\end{itemize}

\subsubsection{Dealing With Negations}
A simple effective workaround by Das and Chen is to prepend \texttt{NOT\_} to every word between a negation and a clause-level punctuation mark.

\subsubsection{More Complications}
\begin{itemize}
	\item Modal adverbs like \emph{quite possibly, totally}
	\item Thwarted expectations "\emph{It was hailed as brilliant, unprecedented artistic achievement worthy of multiple Oscars}"
	\item Non-literal language "\emph{Like 50 hours long}"
\end{itemize}

\subsection{Various Degrees of Complexity}
In principle, all quintuplets in a document should be discovered, which would allow representing unstructured information in a structured form. However, this is extremely hard to do. In practice, simplified forms of sentiment analysis are typically solved
\begin{itemize}
	\item Document-level sentiment classification
	\item Sentence-level sentiment classification
	\item Aspect-level sentiment classification
\end{itemize}
Though simplified, these forms of sentiment analysis are still hard to do with subtlety and sarcasm complicating the task.

\subsubsection{Document-level Sentiment Classification}
Does the document reflect an overall {\color{Green3} positive} or {\color{Firebrick3} negative} view? The approach is the same as for text classification
\begin{enumerate}
	\item tokenisation
	\item feature extraction
	\item classification (Naïve Bayes, MaxEnt, SVM)
	\item Two or Three classes \{ {\color{Green3} positive} $\vert$ {\color{Firebrick3} negative} $\vert$ ({\color{gray} neutral}) \}
\end{enumerate}

\subsection{Generative versus Discriminative Models}
In Naïve Bayes, for a document $d$ and each class $c$:
\begin{itemize}
	\item Estimate $P(d|c)$ and $P(c)$ directly from the training data
	\item Learn a model of $P(d\cap c) = P(d|c)\cdot P(c)$
	\item Use Bayes' rule to compute $P(c|d)=\dfrac{P(d|c)\cdot P(c)}{P(d)}$
	\item Pick the class that maximises $P(c|d)$ over all classes
\end{itemize}
This is called a \textbf{generative model}: Which class $c$ is most likely to have generated my test sample $d$, given the $P(d\cap c)$ that was estimated.

\noindent
The alternative is a \textbf{discriminative model} that models $P(c|d)$ directly.

\subsection{Discriminative Model: Maximum Entropy}
Entropy in information theory refers to a measure of the information content, where highly probable events carry very little information and if all events are equally likely there is maximum entropy. A prediction on which one will occur is impossible and when a event occurs there is a lot of learning.
\begin{itemize}
	\item Suppose movie reviews get classified into \{ {\color{Green3} positive} $\vert$ {\color{Firebrick3} negative} $\vert$ ({\color{gray} neutral}) \}
	\item In the training corpus 90\% of the sample containing the word \texttt{love} are tagged as {\color{Green3} positive}
	\item Let it further be that a test sample with the word \texttt{love} is 90\% likely to be positive and equally likely to be either {\color{Firebrick3} negative} (10\%) or {\color{gray} neutral} (10\%)
	\item All other things unknown are assumed to be equally likely (maximum entropy)
\end{itemize}
Unlike in text classification, topic words are not as important as \textbf{sentiment words} that carry an emotional charge, but the mapping between emotional states and the written word can be exceedingly subtle.
\begin{itemize}[label=-]
	\item {\color{Green3} good}, {\color{Green3} splendid}, {\color{Green3} nice}
	\item {\color{Firebrick3} bad}, {\color{Firebrick3} racist}, {\color{Firebrick3} dismal}
\end{itemize}

\subsection{Unsupervised Sentiment Classification}
\begin{enumerate}
	\item Use a Part-Of-Speech tagger to identify phrases with adjectives and adverbs
	\item Estimate the semantic orientation
	\item Classify based on the average semantic orientation
\end{enumerate}
Semantic orientation is computed based on Pointwise Mutual Information (PMI)
\begin{equation*}
\text{PMI} (w_1,w_2) = \log_2 \left( \frac{P(w_1,w_2)}{P(w_1)P(w_2)} \right) = \log_2 \left( \frac{P(w_1 | w_2)}{P(w_1)} \right) = \log_2 \left( \frac{P(w_2 | w_1)}{P(w2)} \right)
\end{equation*}
PMI is computed using approximate probabilities with normalised word counts in a corpus. Or the web can be used as a corpus using the number of hits from a search engine when $w_1$ and $w_2$ are searched. Following Barrière (2016) the number of hits of \texttt{the, a, of} are used, which results in a total of $2.527\cdot 10^{10}$ hits.
\subsubsection{Example PMI of Academy and Awards}
\begin{itemize}
	\item $w_1 = \text{\texttt{Academy}}$, $w_2 = \text{\texttt{Awards}}$
	\item $P(w_1,w_2) \propto \text{\#\texttt{Academy} AND \texttt{Awards}} \approx 1.01\cdot 10^9\text{hits}$
	\item $P(w_1) \propto \text{\#\texttt{Academy}} \approx 1.94\cdot 10^9 \text{hits}$
	\item $P(w_2) \propto\text{\#\texttt{Awards}} \approx 5.57\cdot 10^9\text{hits} $
	\item $\text{PMI}(w_1,w_2) \approx \log_2\left( \frac{1.01\cdot 10^9\cdot2.527\cdot 10^{10}}{1.94\cdot 10^9\cdot 5.57\cdot 10^9} \right)\approx 1.24 $
\end{itemize}
Two words with high PMI are very likely to occur jointly, while two words with low PMI are very likely to occur separately. PMI can also be used to compute Semantic Orientation (SO), by computing the PMI of selected bigrams with reference words.

\section{Representation and Querying}
The user has information needs and expresses them as a text query, the system matches the query with the documents and returns the results.
The system is using system-specific document or query representations to make comparison possible. The problem is, that computers have difficulties to work with text directly, so the text has to be transformed into something the machine can work with, usually a vector of numbers.

\subsection{Document Representation}
The idea is to count the words that are shared between documents. The higher the score, the more words are shared, the higher the apparent similarity between documents. But lexemes and capitalisation makes this task harder.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/preprocessing_pipeline}
	\caption{Preprocessing pipeline for text}
	\label{fig:preprocessingpipeline}
\end{figure}
The actual steps taken in the preprocessing depends heavily on the use case.

\subsubsection{Clean Text}
\begin{itemize}[noitemsep]
	\item Remove markup
	\item Expand contractions
	\item TrueCase
	\item Remove stop-words
	\item Only keep specific word forms
\end{itemize}

\subsubsection{Tokenise}
\begin{itemize}[noitemsep]
	\item Tokenise
	\item Lemmatisation
	\item Stemming\\
	Reduces inflected words to their stem, the root or base forms, even if the stem itself is not a valid word in the language
\end{itemize}

\subsection{Scoring with Jaccard Similarity}
The idea to count the words that are shared between documents to account for similarity stays the same. But the Jaccard Similarity is defined as the size of intersection divided by the size of the union of two sets.

\noindent
\begin{minipage}{0.55\linewidth}
	\begin{equation*}
	J(\text{document}_1,\text{document}_2) = \frac{\text{document}_1\cap\text{document}_2}{\text{document}_1\cup\text{document}_2}
	\end{equation*}
\end{minipage}
\hfill
\begin{minipage}{0.4\linewidth}
	\begin{center}
		\includegraphics[width=\linewidth]{img/jaccard_set}
	\end{center}
\end{minipage}
Jaccard similarity has the issue of preferring short documents and that it ignores the number of times a word is occurring.

\subsection{Bag of Words Model}
Focuses on the number of occurrence of words, with the intuition that if a queried word occurs more in the document it is more relevant.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/bagofwords_model}
\end{center}
However the Bag of Words model does not consider the ordering of words in a document, and the words must precisely match.

\subsection{Term Frequency Weighting and Transformation}
The raw term frequency is not necessarily the most desirable attribute. A document where a query word occurs more should be handled as more relevant, but a document with 50 occurrences should not be treated 50 times more important than a document with only five occurrences. Thus the relevance should increase sublinear with term frequency
\begin{equation*}
	\text{TF}_{\text{document}} = \underset{\text{word}\in\text{query}\cap\text{document}}{\sum}\log\left( 1 + \text{count}(\text{word},\text{document}) \right)
\end{equation*}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/term_frequency_transformation}
	\caption{A sublinear transformation avoids the dominance of single words}
	\label{fig:termfrequencytransformation}
\end{figure}

\subsubsection{(Best Matching) BM25 Term Frequency Transformation}
\begin{equation*}
	y = \frac{(k+1)\cdot c(w,d)}{k+c(w,d)}
\end{equation*}

\subsection{Inverse Document Frequency}
The problem with term frequency lies in the fact that highly frequent words dominate the analysis, but rare and domain specific words may be more informative semantically. Thus positive weights for frequent words but lower weights than for rare words are desirable.
\begin{itemize}[label=]
	\item $\text{DF}_w$: Document Frequency, the number of documents containing word $w$, is the inverse measure of the informativeness of word $w$
	\item $N$: total number of documents in the collection
\end{itemize}
\begin{equation*}
	\text{IDF}_{word}=\log\left(\frac{N}{\text{DF}_{word}}\right)
\end{equation*}
The logarithm is used to dampen the effect of IDF.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/popular_terms_penalty}
	\caption{Popular terms are penalised}
	\label{fig:populartermspenalty}
\end{figure}

\subsection{TF-IDF}
\begin{equation*}
	\text{TF-IDF}_{word} = \text{TF}_{word,doc}\cdot\text{IDF}_{word,coll}
\end{equation*}
Increases with the number of word occurrences within a document and the rarity of the word in the collection. Ranking of documents for a query is done by
\begin{equation*}
	\text{Score}_{query,doc} = \underset{word\in query\cap doc}{\sum} \text{TF-IDF}_{word,doc,coll}
\end{equation*}
\begin{minipage}{\textwidth}
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\linewidth}{|l|X|p{2cm}|l|X|}
		\cline{1-2}\cline{4-5}
		& TF weights & & & IDF weights\\
		\cline{1-2}\cline{4-5}
		Binary & $ 0 / 1$& & Unary & 1\\
		\cline{1-2}\cline{4-5}
		Counts & $c(t,d)$ & & IDF & $\log\left(\frac{N(C)}{n(t,C)}\right)$\\
		\cline{1-2}\cline{4-5}
		Frequency & $\frac{c(t,d)}{\sum_{\tau\in d} c(\tau,d)}$ & & IDF.
		* & $ 1 + \log\left(\frac{N(C)}{n(t,C)}\right) $ \\
		\cline{1-2}\cline{4-5}
		Log Counts & $ 1 + \log\left( c(t,d) \right) $ & & IDF Smoothed & $\log\left(\frac{N(C)}{1 + n(t,C)}\right)$\\
		\cline{1-2}\cline{4-5}
		\vdots & \vdots & & \vdots & \vdots\\
		\cline{1-2}\cline{4-5}
	\end{tabularx}
\end{minipage}

\subsection{Document Length Normalisation}
Penalise long documents with a document length normaliser, as long documents have a better chance to match any query and are thus less specific. But avoid over-penalisation as documents can be long since they use more words, which should result in more penalisation, or as they have more content, which should be less penalised.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/pivot_length_normaliser}
	\caption{Average document length is used as a pivot point}
	\label{fig:pivotlengthnormaliser}
\end{figure}

\subsubsection{State of The Art Okapi BM25+}
Given a Query $Q$ containing keywords $q_1,\dots,q_n$, document $D$'s score is
\begin{equation*}
	\text{score}(D,Q) = \sum_{i=1}^{n}\text{IDF}(q_i) \cdot \left[ \frac{f(q_i,D)\cdot (k_1 + 1)}{f(q_i, D) + k_1\cdot\left( 1 - b + b\cdot\frac{\abs{D}}{\text{avgdl}} \right)} + \delta \right]
\end{equation*}
\begin{equation*}
	\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
\end{equation*}
\begin{itemize}[leftmargin=*, labelindent=2cm, labelsep=1cm, noitemsep,nosep]
	\item[$f(q_i,D)$] Term frequency of $q_i$ in document $D$
	\item[$\abs{D}$] length of document $D$
	\item[avgdl] average document length over corpus
	\item[$\delta$] free parameter, default $\delta = 1.0$
	\item[$k_1$] free parameter, $k_1 \in [1.2, 2.0]$
	\item[$b$] free parameter, default $b = 0.75$
	\item[$N$] number of documents in corpus
	\item[$n(q_i)$] number of documents containing $q_i$
\end{itemize}

\subsection{Vector Space Model}
Used to correlate words, sentences and documents by semantic similarity.
\begin{center}
	\includegraphics[width=0.5\linewidth]{img/vector_space_model}
\end{center}
For scoring the cosine similarity between documents and a query vector is preferred over the euclidean distance
\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=1cm, noitemsep]
	\item[similar scores] score vectors are in the same direction, angle between them is almost zero making the cosine lying near one or 100\%
	\item[unrelated scores] score vectors are nearly orthogonal and the cosine is thus near zero
	\item[opposite scores] score vectors in opposite directions, cosine is near minus one or -100\%
\end{itemize}         
The cosine similarity is calculated by
\begin{equation*}
	\cos(d_j,q) = \frac{\bm{d_j}\cdot \bm{q}}{\norm{\bm{d_j}}\norm{\bm{q}}} = \frac{\sum_{i=1}^{N} w_{i,j} w_{i,q} }{\sqrt{\sum_{i=1}^{N}w_{i,j}^2}\sqrt{\sum_{i=1}^{N}w_{i,q}^2}}
\end{equation*}
$\text{L}^2$ norm (Euclidean norm) is the square root of the dot product of the vector $\bm{p}$ with itself, and scales a vector to its unit-length
\begin{equation*}
	\norm{\bm{p}} = \sqrt{p_1^2 + p_2^2 + \dots + p_n^2} = \sqrt{\bm{p}\cdot\bm{p}}
\end{equation*}
Normalising all vectors to unit vectors this way allows for the dot-product to measure similarity, and do it in a more efficient way than cosine similarity. The dot product between two vectors $\bm{d_j}$ and $\bm{q}$ is high if $w_{ij}$ and $w_{iq}$ have similar characteristics, and is equivalent to the cosine similarity if the vectors are normalised.

\section{Word Embeddings}
An embedding is a \textbf{representation of words as dense vectors} such that the properties of the words and the relationships between the words are preserved. \textbf{Count-based methods} compute statistics of word co-occurrence in large text corpora, and then map these count-statistics down to a small, dense vector for each word. Predictive models use a (shallow) neural network to predict
words from its neighbours. The learned weights act as word vector representations.

Word embeddings are automatically learned and allow for vector-oriented reasoning based on the offsets between words.

\subsection{Context-Counting Word Vectors}
Represent mutual information of a word with other words by global co-occurrence counts using a pre-defined window size.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/context_counting}
\end{center}
The problem is that this matrix increases with vocabulary and windows size, which results in a very high dimensional, but sparse feature matrix. Single value decomposition (SVD) is used to project the co-occurrence matrix to a smaller dimension and this sub-matrix is taken as the word embedding matrix.

\subsection{Word2Vec}
Word2Vec is a simple and computationally efficient way to learn embeddings. It is a prediction based method and comes in two flavours, the Continuous Bag of Words (CBOW) and the Skip-gram model.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/word2vec_model}
\end{center}

\subsubsection{CBOW}
\begin{minipage}{0.45\linewidth}
	Given a set of context words, predict the centre word that potentially occurs with the context words. The input are $m$ one-hot vectors of context window words.
	\vspace{5em}
\end{minipage}
\begin{minipage}{0.5\linewidth}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{img/word2vec_cbow}
	\end{center}
\end{minipage}

\subsubsection{Skip-Gram}
\begin{minipage}{0.55\linewidth}
	Given a center word, predict context words that potentially occur with 	the centre word. Input is the one-hot vector for word $w$.
	\vspace{10em}
\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{img/word2vec_skip-gram}
	\caption{Source: \href{https://inspirehep.net/literature/1630982}{An Information Retrieval and Recommendation System for Astronomical Observatories}}
	\label{fig:word2vecskip-gram}
\end{figure}
\end{minipage}
\begin{center}
	\includegraphics[width=0.9\linewidth]{img/word2vec_example}
\end{center}

\subsubsection{Softmax}
\begin{equation*}
	\hat{y} = p({\color{SteelBlue4}o}|{\color{red}c}) = \frac{e^{{\color{red}v_c^T}{\color{SteelBlue4} u_0}}}{{\color{SpringGreen4}\sum_{w\in V}} e^{{\color{red}v_c^T}{\color{SteelBlue4} u}_{\color{SpringGreen4} w}}}
\end{equation*}
Inner product ${\color{red}v_c^T}{\color{SteelBlue4} u_0}$
\begin{itemize}[label=-]
	\item ${\color{red}v_c^T}$ := embedding of word $w$ if word is {\color{red} center} word
	\item ${\color{SteelBlue4} u_0}$ := embedding of word $w$ if word is {\color{SteelBlue4} contex} word
	\item Measures the similarity of the two embedding representations of ${\color{red}v_c^T}$ and ${\color{SteelBlue4} u_0}$
\end{itemize}
The exponentiation makes all values positive and large values even larger, while the division by the {\color{SpringGreen4} sum over all words in the vocabulary} makes $\hat{y}$ a probability.

\subsection{GloVe}
Combines count based co-occurrence statistics with predictive context window method. GloVe is faster to train than Word2Vec and scales to huge corpora.

\subsubsection{Out of Vocabulary Words}
While using Word2Vec or GloVe out-of-vocabulary words will be encountered, as the possible vocabulary is infinitely large. Subword methods are based on the assumption that the semantic representation can be reconstructed from known parts of an unknown word.

FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. Each word is represented with a bag of $n$-grams in addition to the
word itself. It has proven to be more accurate than Word2Vec, but models are quite big and require a lot of computation for training.

\subsection{Practical Advice}
Use pre-trained word embeddings, unless there is a corpus of billions of words in the training set, where it is appropriate to start randomised. If your training set is small do not fine-tune the word embeddings. If the training set is large (more than one million words or with domain specific content), it probably will work better when fine-tuned to the task. It is generally advisable to compare results for both methods.

\section{Word Sense Disambiguation}

\subsection{Homonyms}
\begin{itemize}[leftmargin=*, labelindent=2cm, labelsep=1cm]
	\item[Homographs] words written the same way
	\item[Homophones] words that sound the same
\end{itemize}
In general Homographs cause problems with machine translation, while Homophones cause problems with text-to-speech. Words exhibiting these characteristics are called polysemous; words with multiple related senses.

\subsubsection{Zeugma Test}
Apply a word to two others in different senses and test if it makes sense.

"Routers are used to cut channels in wood and to connect different networks."

\subsection{Synonyms and Antonyms}
Synonyms are lexical items that have the same meaning in some or all contexts. Perfect synonymy seldom occurs, one or a few lexemes are typically preferable to other synonyms, and may convey the intended meaning better. This could be notions of politeness, register, acceptability, convention or nuance.

For example \textit{automobile} is more formal than \textit{car}.

\vspace{1em}
\noindent
Antonyms are words with opposite senses
\begin{itemize}[leftmargin=*, labelindent=4cm, labelsep=1cm]
	\item[Reversives] rise/fall, in/out, up/down
	\item[Opposite ends of a scale] long/short, hot/cold
\end{itemize}

\subsection{Hyponymy and Hypernymy}
One sense is a hyponym of another if it denotes a subclass. Sense A is a hyponym of sense B if being an A entails being a B.
\begin{itemize}[label=-]
	\item A:B = hyponym:hypernym = subtype:supertype
	\item Also known as the IS-A relationship
\end{itemize}
Hypernymy is the opposite relationship.

\subsection{Meronymy and Holonymy}
Part-whole and whole-part relationship. A meronym is a part of its holonym, for example a \textit{wheel} is a meronym of \textit{car}. There is a HAS-A relationship between a holonym and its meronym.

\subsection{Word Similarity Based on WordNet}
Look at the shortest path between sense nodes.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/WordNet_similarity}
\end{center}

\subsection{Word Vectors and Word Meaning}
\begin{itemize}
	\item One-hot vectors: No notion of word meaning
	\item Co-occurence matrices: words are represented by the weighted occurrence of other words in their context
	\item Pointwise mutual information (PMI) matrices: each element $(i,j)$ signifies whether words $i$ and $j$ are more likely to occur jointly than to be independent
	\begin{itemize}
		\item Negative PMI tends to be unreliable, as words co-occur less than expected with a corpus too small
		\item In practice positive PMI (PPMI) is used
	\end{itemize}
\end{itemize}
\begin{equation*}
	\text{PPMI}(w_1,w_2) = \max{\left(\log_2{\left(\frac{P(w_1,w_2)}{P(w_1)P(w_2)}\right)}\right)}
\end{equation*}

\subsection{Word Sense Disambiguation}
\textbf{Extrinsic} evaluation
\begin{itemize}
	\item Embed WSD in another task
	\item Measure the task performance without and with WSD and see whether it improves
\end{itemize}
\textbf{Intrinsic} evaluation
\begin{itemize}
	\item Use a labeled corpus
	\item Measure the accuracy, evaluated based on held-out data from the labelled corpus
\end{itemize}
As a baseline choose the most frequent sense or use the Lesk Algorithm.

\subsubsection{Simplified Lesk Algorithm}
The principle is to choose sense with most word overlap between gloss and context. Disambiguate the word bank in the sentence:

"The bank can guarantee that deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities."

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/simplified_lesk_algorithm}
\end{center}

\subsubsection{Word Sense Disambiguation Based on Word Embeddings}
Embedding vectors like Word2Vec or GloVe can be used for word sense disambiguation.
\begin{itemize}
	\item Capture context of each word sense
	\begin{itemize}
		\item Find the gloss of the word sense in WordNet
		\item Get the embedding vectors of the words in the gloss
		\item Average out the embedding vectors and obtain an embedding vector for the context of each target word sense
	\end{itemize}
	\item Do the same for the sentence containing the target word
	\item Compare the embedding vector  for the target sentence to the embedding vectors for the target word senses
	\item Pick the word sense whose embedding vector is the most similar to the one of the target sentence
\end{itemize}

\begin{minted}[linenos]{python}
for sense in range (1, len(word_vectors)):
	print ('context '+ str(sense) + ': ' + get_gloss(target_word, sense)) 
\end{minted}
\begin{minted}[breaklines]{text}
context 1: sloping land (especially the slope beside a body of water) 
context 2: a financial institution that accepts deposits and channels the money into lending activities 
context 3: a long ridge or pile 
context 4: an arrangement of similar objects in a row or in tiers 
context 5: a supply or stock held in reserve for future use (especially i n emergencies) 
context 6: the funds held by a gambling house or the dealer in some gambl ing games 
context 7: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force 
context 8: a container (usually with a slot in the top) for keeping money at home 
context 9: a building in which the business of banking transacted 
context 10: a flight maneuver; aircraft tips laterally about its longitud inal axis (especially in turning) 
\end{minted}

\section{Part-of-Speech Tagging}
PoS tagging means identifying the grammatical category of each word token.

\vspace{1em}
\noindent
\begin{minipage}{0.45\linewidth}
	\begin{itemize}
		\item nouns
		\item verbs
		\item adjectives
		\item adverbs
		\item preposition
	\end{itemize}
\end{minipage}
\begin{minipage}{0.45\linewidth}
	\begin{itemize}
		\item particles
		\item conjunctions
		\item pronouns
		\item numerals
		\item interjections
	\end{itemize}
\end{minipage}

\subsection{Corpora}
There are corpora with manually tagged PoS, which are both in part in NLTK
\begin{itemize}
	\item Brown corpus
	\item Penn Treebank
\end{itemize}
The Penn Treebank tags are widely used for English language PoS tagging.

\noindent
\begin{minipage}[t][][t]{0.45\linewidth}
	\begin{tabularx}{\linewidth}{c|c|X}
		\textbf{Number} & \textbf{Tag} & \textbf{Description} \\
		\hline
		1 & CC & Coordinating conjunction \\
		2 & CD & Cardinal number \\
		3 & DT & Determiner \\
		4 & EX & Existential there \\
		5 & FW & Foreign word \\
		6 & IN & Preposition or subordinating conjunction \\
		7 & JJ & Adjective \\
		8 & JJR & Adjective, comparative \\
		9 & JJS & Adjective, superlative \\
		10 & LS & List item marker \\
		11 & MD & Modal \\
		12 & NN & Noun, singular or mass \\
		13 & NNS & Noun, plural \\
		14 & NNP & Proper noun, singular \\
		15 & NNPS & Proper noun, plural \\
		16 & PDT & Predeterminer \\
		17 & POS & Possessive ending \\
		18 & PRP & Personal pronoun \\
		19 & PRP\$ & Possessive pronoun
	\end{tabularx}
\end{minipage}
\begin{minipage}[t][][t]{0.45\linewidth}
	\begin{tabularx}{\linewidth}{c|c|X}
		\textbf{Number} & \textbf{Tag} & \textbf{Description} \\
		\hline
		20 & RB & Adverb \\
		21 & RBR & Adverb, comparative \\
		22 & RBS & Adverb, superlative \\
		23 & RP & Particle \\
		24 & SYM & Symbol \\
		25 & TO & to \\
		26 & UH & Interjection \\
		27 & VB & Verb, base form \\
		28 & VBD & Verb, past tense \\
		29 & VBG & Verb, gerund or present participle \\
		30 & VBN & Verb, past participle \\
		31 & VBP & Verb, non-3rd person singular present \\
		32 & VBZ & Verb, 3rd person singular present \\
		33 & WDT & Wh-determiner \\
		34 & WP & Wh-pronoun \\
		35 & WP\$ & Possessive wh-pronoun \\
		36 & WRB & Wh-adverb
	\end{tabularx}
\end{minipage}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{img/open_close_classes}
	\caption{Open and closed classes}
	\label{fig:opencloseclasses}
\end{figure}

\subsection{English Nouns}
\begin{itemize}
	\item Count nouns allow for grammatical enumeration
	\item Mass nouns are used for a homogeneous group of concepts which are not countable
	\item Proper nouns are used to mane people, places or things, and are generally capitalised
\end{itemize}

\subsection{English Verbs}
\begin{itemize}
	\item Main verbs refer to actions and processes
	\item Auxiliary or Modal verbs change the meaning of main verbs
	\item There are four inflections
	\begin{itemize}
		\item Non-third person singular (VBP)
		\item Third person singular (VBZ)
		\item Progressive (VBG)
		\item Past participle (VBN)
	\end{itemize}
\end{itemize}

\subsection{Other English Part of Speech Components}
\begin{itemize}
	\item Adjectives characterise nouns
	\item Adverbs characterise verbs, adjectives, other adverbs, or word groups, they are separated into directional, degree, manner or temporal adverbs
	\item Prepositions specify relationships, be that spatial, temporal or agency\\
	\emph{arrive at the theatre on time}
	\item Particles look like prepositions or adverbs but serve to modify the meaning of the verb that precedes them\\
	\emph{to find out}
	\item Determiners appear directly before a noun
	\begin{itemize}
		\item definite article: the
		\item indefinite article: a, an
		\item demonstratives: this, that, these, those
	\end{itemize}
	\item Conjunctions join clauses
	\begin{itemize}
		\item coordinating conjunctions link clauses of equal importance\\
		\emph{I respect his opinion but in this case he's factually wrong.}
		\item subordinating conjuctions a less important clause to complement a more important clause
	\end{itemize}
\end{itemize}

\subsection{Pronouns}
\begin{itemize}
	\item Relative pronouns introduce subordinate clauses that modify their antecedent
	\item Interrogative pronouns introduce questions
	\item Demonstrative pronouns identify or point at nouns
	\item Indefinite pronouns refer to nonspecific persons or things
\end{itemize}

\subsection{Handling Ambiguity}
For each ambiguous word token, choose the tag that is most frequent for that word token in the training corpus.
\begin{itemize}
	\item PRP+VBD is very common, while PRP+VBN is exceedingly uncommon
	\item RB+RB is very common, also at the end of a sentence
	\item RB+JJ is also very common, but not at the end of a sentence
	\item IN+JJ and IN+RB are both fine, but there would have to be more word tokens afterwards
\end{itemize}

\subsection{PoS Tagging as Sequence Classification}
A sentence is a sequence of observations, thus tagging can use probabilistic methods to choose the most likely tag sequence. In practice, each token is classified independently while using information about the surrounding tokens as input features. For ambiguous tags the forward and backward sequence of the already identified tags can be used.

\subsection{Markov Chains}
\textbf{Markov chains} are models that identify the underlying probabilities of a sequence of random variables.
\begin{itemize}
	\item a set of $N$ state variables
	\item a transition probability matrix $A$, where $a_{i,j}$ is the probability of going from state $i$ to state $j$
	\item an initial probability distribution over the $N$ states
	\begin{itemize}
		\item this is what is known \emph{a priori} before observing the system
		\item $\pi_i$ is the probability that the chain will start in state $i$
	\end{itemize}
\end{itemize}
A key assumption is that the next state only depends on the current state. For word sequences, this means that the assumption is that the next word only depends on the current word.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/two_state_markov_model.png}
	\caption{Two-state Markov Chain Model \parencite{markov}}
\end{figure}

Notice that the probabilities sum to one for each state ($ \alpha + (1-\alpha) = 1$). The transition matrix $A$ for this system is given by
\begin{equation*}
	\begin{pmatrix}
	1-\alpha & \alpha\\
	\beta & 1-\beta
	\end{pmatrix}
\end{equation*}
$n$ steps of a Discrete State Markov Chain (DSMC) can be simulated simply b y repeated multiplication of the transition matrix with itself.

Unfortunately Markov Chains only work if the interest lies in the probability of observed events. In PoS tagging, interest lies in the probability of things that can't be observed directly. Namely, these are the tags, which are hidden. Fortunately \textbf{Hidden Markov Chains} enable the modelling of sequences with both observed events and related hidden events. In PoS tagging these correspond to words and tags.
\begin{itemize}
	\item a set of $N$ states
	\item a $N\times N$ \textbf{transition matrix} $A$ where $a_{i,j}$ is the probability of transitioning from state $i$ to state $j$
	\item a sequence $O$ of $T$ observations (drawn from a set of $V$ possible observations): $O = o_1, o_2, \dots, o_T$
	\item a $N\times V$ \textbf{emission matrix} $B$ where $b_{i,t} = P\left(o_t\middle| i\right)$ is the probability of state $i$ generating observation $o_t$
	\item an initial probability distribution over the $N$ states
\end{itemize}
Let $W$ be an observed word sequence and $T$ be a hidden tag sequence. The interest lies in $\hat{T}$ over all possible tag sequences.
\begin{equation*}
	\hat{T} = \arg\max P(T|W) =\arg\max P(W|T)P(T) = \arg\max \prod_{i} P\left( w_i \middle| t_i \right) \prod_{i} P\left( t_i \middle| t_{i-1} \right)
\end{equation*}
The process used to discover the tag sequence that maximizes $P(W|T)P(T)$ is called decoding and can be carried out using the Viterbi algorithm.

\subsection{Viterbi Algorithm for Hidden Markov Models}
The Hidden Markov Model may be represented as a graph called a trellis with a row for each tag and a column for each word
\begin{center}
	\includegraphics[width=0.5\linewidth]{img/vitrelli_trellis}
\end{center}
The $i$th column corresponds to the $i$th observation, the $k$th row corresponds to the $k$th hidden stage. The goal is to find the most probable path through the trellis, which refers to the path with the highest end-to-end probability computed as the products of the weights along the path.

The weight of the arrow from node $(k,i)$ to node $(j,i+1)$ is the product of two probabilities
\begin{itemize}[noitemsep,nosep]
	\item the probability of going from state $k$ to $j$\qquad $a_{k,j} = P(j|k)$
	\item the probability of state $j$ generating word $i+1$ \qquad $b_{j,i+} = P(i+1|j)$
\end{itemize}
The path through the trellis with the highest end-to-end probability is the most likely tag sequence associated to the word sequence.

\begin{center}
	\includegraphics[width=0.6\linewidth]{img/vitrelli_trellis_example}
\end{center}

\section{Language Models}
Language modelling is the task of predicting what word follows a sequence of words. The formal definition is: given a sequence of words $w^{(1)}, w^{(2)},\dots, w^{(t-1)}$ a language model computes the probability distribution of the next word $w^{(t)}$
\begin{equation*}
	P\left(w^{(t)} \middle| w^{(t-1)}, \dots, w^{(1)} \right)
\end{equation*}
where $w^{(t)}$ can be any word in a fixed vocabulary $V = (w_1,\dots,w_{\abs{V}})$.

We can also see a Language Model as a system that assigns a probability to any sequence of words. According to the chain rule the probability of a text $w^{(1)},\dots, w^{(T)}$ is given by
\begin{align*}
	P\left(w^{(1)},\dots, w^{(T)}\right) &= P\left(w^{(1)}\right) \cdot P\left(w^{(2)} \middle| w^{(1)}\right) \cdot ... \cdot P\left(w^{(T)} \middle| w^{(T-1)},\dots, w^{(1)}\right)\\
	&= \prod_{t=1}^{T} P\left( w^{(t)} \middle| w^{(t-1)}, \dots, w^{(1)} \right)
\end{align*}
The exact order of the words matters.

\subsection{n-Gram Language Model}
Some simplifications are needed for this idea to be practicable. Thus, the probability of a word $w^{(t)} \cdot P\left(w^{(t)}\right)$ only depends on the $n-1$ preceding words. This simplification is called a $n$-gram model where $n$-grams are chunks of $n$ consecutive words. Each component is approximated with the following product
\begin{align*}
	P\left( w^{(1)}, \dots, w^{(T)} \right) &\approx \prod_{t=1}^{T} P\left( w^{(t)} \middle| w^{(t-1)},\dots, w^{(i-(n-1))} \right)\\
	P\left( w^{(t)} \middle| w^{(t-1)},\dots, w^{(1)} \right) &\approx P\left( w^{(t)} \middle| w^{(t-1)},\dots, w^{(i-(n-1))} \right)
\end{align*}
The $n$-gram probabilities are based on the Maximum likelihood estimate
\begin{equation*}
	P\left( w^{(t)} \middle| w^{(t-1)},\dots, w^{(1)} \right) \approx \frac{\text{count}\left(w^{(t)}, w^{(t-1)},\dots, w^{(t-(n-1))}\right)}{\text{count}\left(w^{(t-1)},\dots, w^{(t-(n-1))}\right)}
\end{equation*}

\subsubsection{Problems}
\begin{itemize}
	\item The context for a word is bigger than the $n$ grams in the model
	\item Increasing $n$ increases the sparsity of the model (typically $n$ can't be bigger than five)
	\begin{equation*}
		P\left( w^{(t)} \middle| w^{(t-1)},\dots, w^{(1)} \right) \approx \frac{\text{count}\left(w^{(t)}, w^{(t-1)},\dots, w^{(t-(n-1))}\right)}{\text{count}\left(w^{(t-1)},\dots, w^{(t-(n-1))}\right)}
	\end{equation*}
	\item Numerator $w^{(1)},\dots, w^{(t-1)},w^{(t)}$ might never occur in the training set\\
	Solution: Smoothing
	\item Denominator $w^{(1)},\dots, w^{(t-1)}$ might never occur in the training set\\
	Solution: Backoff
\end{itemize}

\subsubsection{Smoothing n-Gram Language Models}
It can always happen that n-grams not seen in the training set appear in the data, Smoothing takes care of this by reassigning some probability to unseen $n$-grams. The simplest method for this is \textbf{Laplace smoothing}
\begin{equation*}
	P\left( \bm{w^{(t)}} \middle| w^{(t-1)} \right) \approx \frac{\text{count}\left(w^{(t-1)},w^{(t)}\right) + 1}{\text{count}\left(w^{(t-1)}\right) + \abs{V}}
\end{equation*}
Smoothing this way generally shifts the probability too much towards unseen $n$-grams. Compensate this by adding a fraction $k$ (0.01, 0.03, 0.1, 0.3) instead of one. This method is called \textbf{add-$\bm{k}$ smoothing}
\begin{equation*}
	P\left( \bm{w^{(t)}} \middle| w^{(t-1)} \right) \approx \frac{\text{count}\left(w^{(t-1)},w^{(t)}\right) + k}{\text{count}\left(w^{(t-1)}\right) + k\cdot\abs{V}}
\end{equation*}
Unfortunately, neither add-one nor add-$k$ work well in practice.

\subsubsection{Backoff in n-Gram Language Models}
Use the count of the next lower-order $n$-gram, which means to use less context. Even better is to take backoff into account when n-grams are sparse, so use a linear combination of $n$-grams
\begin{equation*}
	P\left( w^{(t)} \middle| w^{(t-1)},w^{(t-2)} \right) = \lambda_1 P\left( w^{(t)} \middle| w^{(t-1)},w^{(t-2)} \right) + \lambda_2 P\left( w^{(t)} \middle| w^{(t-1)} \right) + \lambda_3 P\left( w^{(t)} \right)\qquad \sum_{i} \lambda_i = 1
\end{equation*}

\subsection{Evaluation of a Language Model}
\begin{itemize}
	\item Extrinsic Evaluation
	\begin{itemize}
		\item Put model A and model B through the same task
		\item Run the task and get the accuracy for both models
		\item Compare accuracies
	\end{itemize}
	Cons: Time Consuming
	\item Intrinsic Evaluation
	\begin{itemize}
		\item Easier and faster to use
		\item Evaluation metric: Perplexity
		\begin{itemize}
			\item Algorithm that scores better on the test set is considered better
		\end{itemize}
	\end{itemize}
	Cons: Bad approximation for real language (but it helps to get an idea on how
	good the model works)
\end{itemize}

\subsubsection{Perplexity}
Perplexity is the probability of the test set, normalized by the number of words
\begin{flalign*}
	PP(W) = P(w^{(1)},w^{(2)},\dots,w^{(N)})^{-\frac{1}{N}} &= \sqrt[\leftroot{-2}\uproot{2} N]{\frac{1}{P\left(w^{(1)},w^{(2)},\dots,w^{(N)}\right)}} &\\
	&= \sqrt[\leftroot{-2}\uproot{2} N]{\prod_{t=1}^{N}\frac{1}{P\left(w^{(t)}\middle| w^{(t-1)},\dots,w^{(1)}\right)}} & (\text{chain rule})\\
	&= \sqrt[\leftroot{-2}\uproot{2} N]{\prod_{t=1}^{N} \frac{1}{P\left(w^{(t)}\middle| w^{(t-1)}\right)} } & (\text{for bigrams})
\end{flalign*}
Minimizing perplexity is the same as maximizing probability.

Another evaluation metric is the word error rate (the number of insertions, deletions and substitutions normalised by sentence length needed to re-construct a reference sentence). The word error rate can be measured with the Levenshtein Edit Distance (which allows to weight insertions, deletions and substitutions differently) and is efficiently implemented using dynamic programming.

\section{Recurrent Neural Networks and Language Models}
The task of a Language Model is at its core to take as an input a sequence of word $x^{(1)}, x^{(2)}, \dots, x^{(t-1)}$ and output a probability distribution of the next word $P\left( x^{(t)} \middle| x^{(1)}, x^{(2)}, \dots, x^{(t-1)} \right)$.

A fixed-window neural language model uses a fixed input size of preceding words to predict the next word
\begin{center}

	\includegraphics[width=0.6\linewidth]{img/fixed_window_nlm}
\end{center}
This model will provide an output distribution to any word combination (it might not be a good one) and while making the context window size n larger does not lead to larger sparse matrices. The size of the hidden layer is an independent hyper-parameter.

\subsection{Recurrent Neural Network}
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/recurrent_neural_network}
\end{center}
The hidden state vector $h^{(t)}$ has two inputs, the input word $x^{(t)}$ and the previous hidden state $h^{(t-1)}$. The core idea of RNNs is to apply the same weights repeatedly, thus the weight matrix $W_h$ for weighing the previous hidden state vector $h^{(t-1)}$ and weight matrix $W_e$ for weighing the one-hot encoded input word vector $x^{(t)}$ are shared.

\begin{tabularx}{\linewidth}{p{0.45\linewidth} p{0.45\linewidth}}
	{\color{Green3} \textbf{Advantages}} & {\color{Firebrick3} \textbf{Disadvantages}} \\
	\hline
	\begin{itemize}[label={\color{Green3}\small+}, leftmargin=*]
		\item can process input sequences of any length
		\item computation for timestep $t$ can use information from any preceding timestep
		\item model size does not increase for longer input
		\item symmetry in how input is processed, as the same weights are applied for every timestep
	\end{itemize}
	&
	\begin{itemize}[label={\color{Firebrick3}-}, leftmargin=*]
		\item recurrent computation is slow as no parallelisation is possible because inputs have to be handled sequentially by design
		\item difficult to access information from many time steps back in practice
	\end{itemize}
\end{tabularx}
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/rnn_models}
\end{center}
Training a RNN amounts to feeding a big corpus of text into the RNN and computing the output distribution $\hat{y}^{(t)}$ at every step $t$. The loss function on step $t$ is the cross-entropy between predicted probability distribution and the true word
\begin{equation*}
	J^{(t)}(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = -\sum_{w\in V} y_w^{(t)} \log\left(\hat{y}_w^{(t)}\right) = -\log\left(\hat{y}_{x_{t+1}}^{(t)}\right)
\end{equation*}
The average of the loss is the overall loss for the training set
\begin{equation*}
	J(\theta) = \frac{1}{T}\sum_{t=1}^{T}J^{(t)} (\theta) = \frac{1}{T} \sum_{t=1}^{T}\left(-\log\left(\hat{y}_{x_{t+1}}^{(t)}\right)\right)
\end{equation*}
Errors are backpropagated at each individual time step and then across all time steps, in a process that is called backpropagation through time. Computing gradients with regard to $h_0$ involves many factors of $W_h$ and repeated gradient computations, which can lead to the exploding and vanishing gradient problem.

Measures against the problem
\begin{itemize}
	\item \textbf{Exploding Gradients}
	\begin{itemize}
		\item Gradient clipping to scale big gradients
	\end{itemize}
	\item \textbf{Vanishing Gradients}
	\begin{itemize}
		\item Activation Function
		\item Weight initialisation
		\item Network architecture
	\end{itemize}
	see \cite{pascanu2013difficulty} for an in-depth explanation
\end{itemize}

\section{Seq2Seq and Attention for Machine Translation}

Machine Translation is the task of translating a sentence $x$ written in a \textbf{source language} to a sentence $y$ written in the \textbf{target language}.
\begin{itemize}[label=-]
	\item Source sentence represented by input sequence $(x^{(1)},x^{(2)},\dots,x^{(T)})$ of variable length $T$
	\item Target sentence represented by output sequence $(y^{(1)},y^{(2)},\dots,y^{(T')})$ of variable length $T'$
	\item Elements of $x$ and $y$ come from the vocabulary of the respective language
\end{itemize}

\subsection{Statistical Machine Translation}
Learn a probabilistic model from data ${\color{red} P(y|x)}$ by using the Bayes' rule. Break the problem into two probability functions and use statistical models to model each probability function separately
\begin{equation*}
	{\color{DarkOrchid4}\text{argmax}_y} {\color{red} P(y|x)} = {\color{DarkOrchid4}\text{argmax}_y} {\color{blue} P(x|y)}{\color{Green3} P(y)}
\end{equation*}
\begin{itemize}[label=-]
	\item {\color{blue} Translation Model}: Models how words and phrases should be translated ({\color{blue} fidelity}) and is learnt from \textbf{parallel data}
	\item {\color{Green3} Language Model}: Models how to write good language ({\color{Green3} fluency}) and is learnt from \textbf{monolingual data}
	\item {\color{DarkOrchid4} Decoding Algorithm}: Finds the translation that maximises the probabilities of the two models, this is usually done by beam search (a heuristic search algorithm)
\end{itemize}

Learn the translation model ${\color{blue} P(x|y)}$ by starting with a \textbf{large parallel corpus} like the Rosetta Stone. The language model can be broken down further into ${\color{blue} P(x,a|y)}$ where $a$ is the \textbf{alignment}, which is the \textbf{correspondence between particular words} in the translated sentence pair.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{img/alignment_complex}
	\caption{Examples showing the complexity of alignment}
	\label{fig:alignmentcomplex}
\end{figure}

Statistical Machine Translation was a huge research field. But the best systems were extremely complex with hundreds of important detail and separately designed sub-components. A lot of human effort was needed to build and maintain the systems for each language pair.

\clearpage
\subsection{Neural Machine Translation}
Neural Machine Translation is a way to do Machine Translation with a single neural network. The neural network architecture is called sequence-to-sequence and involves two RNNs. An {\color{red} \textbf{Encoder}} network which processes the input sequence and produces a fixed size {\color{Goldenrod2} context vector}, and a {\color{Green3} \textbf{Decoder}} network which takes the context vector as input to produce the translated output word sequence.

\begin{center}
	\includegraphics[width=0.6\linewidth]{img/encoder_decoder_machine_translation}
\end{center}

The idea is to use an end-to-end neural network to directly implement ${\color{red}\text{argmax}_y P(y|x)}$

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/encoder_decoder_machine_translation2}
\end{center}

This sequence to sequence (Seq2Seq) system is trained as single whole and the backpropagation operates from end to end. Using Teacher forcing: Supply expected translation (from corpus) instead of predicted output from previous timestep.

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/Seq2Seq_Teacher_Forcing}
\end{center}

\subsection{Scoring a Target Sequence}
The output of the Decoder for the next word is a multinational probability distribution over the vocabulary. The idea behind the scoring is to maximise the probability for the complete translation of length $T$
\begin{align*}
	P(y|x) &= P(y^{(1)}|x) \cdot P(y^{(2)}|y^{(1)},x) \cdot\ldots\cdot \underbrace{P(y^{(T)}|y^{(1)},y^{(2)},\dots,y^{(T-1)},x)}_{\text{probability of next target word given words so far and source sentence $x$}}\\
	P(y|x) &= \prod_{t=1}^{T} P(y^{(t)}|y^{(1)},y^{(2)},\dots,y^{(t-1)},x)
\end{align*}

For numerical stability the logarithm is used to score a candidate solution
\begin{align*}
	\text{score}\left(y^{(1)},\dots,y^{(t)})\right) &= \log\left( \prod_{t=1}^{T} P(y^{(t)}|y^{(1)},y^{(2)},\dots,y^{(t-1)},x) \right)\\
	&= \sum_{t=1}^{T} \log\left(P(y^{(t)}|y^{(1)},y^{(2)},\dots,y^{(t-1)},x)\right)
\end{align*}

\clearpage

\subsection{Beam Search}
On each step of the decoder, keep track of the $k$ \textbf{most probable partial translations}, which are called hypotheses. Beam search is a heuristic search strategy, where $k$ is called the beam size (in practice around 5 to 10).

In order to choose the best $k$ hypotheses for the next word in time step $t$ we score the alternatives.
\begin{equation*}
	\text{score}\left(y^{(1)},\dots,y^{(t)}\right) = \sum_{t=1}^{T} \log\left( y^{(t)} \middle| y^{(1)},\dots,y^{t-1}, x \right)
\end{equation*}
Scores are all negative, a higher score is better. Search for a high-scoring hypothesis while tracking the top $k$ on each step.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search01}
		\caption{Step 1}
	\end{subfigure}
	\hspace{1em}
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search02}
		\caption{Step 2}
	\end{subfigure}
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search03}
		\caption{Step 3}
	\end{subfigure}
	\hspace{1em}
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search04}
		\caption{Step 4}
	\end{subfigure}
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search05}
		\caption{Step 5}
	\end{subfigure}
	\hspace{1em}
	\begin{subfigure}[h]{0.45\linewidth}
		\includegraphics[width=0.8\linewidth]{img/beam_search06}
		\caption{Step 6}
	\end{subfigure}
	\caption{Beam Search full example}
\end{figure}

\clearpage

\textbf{\begin{figure}[htb]
		\centering
		\includegraphics[width=0.7\linewidth]{img/beam_search07}
		\caption{Full hypothesis of Beam Search}
		\label{fig:beamsearch07}
	\end{figure}
}

\subsection{Hypotheses Selection}
\begin{itemize}
	\item In \textbf{greedy decoding}, the input gets decoded until the model produces the \textbf{\texttt{<END>} token}
	\item In \textbf{beam search decoding} different Hypotheses may produce the \texttt{<END>} token in different timesteps, if that happens the hypothesis is saved and other hypotheses are continued to be explored until a predefined cut-off at timestep $T$ is reached or until at least $n$ complete hypotheses are acquired.
	
	Then the hypothesis with the best score is selected, but longer hypotheses have lower scores (normalised by length)
	\begin{equation*}
		{\color{red}\frac{1}{T}} \sum_{t=1}^{T} \log\left(P(y^{(t)} | y^{(1)},\dots, y^{(t-1)}, x)\right)
	\end{equation*}
\end{itemize}

\subsection{Evaluating Machine Translation Systems using BLEU}
The \textbf{B}i\textbf{l}ingual \textbf{E}valuation \textbf{U}nderstudy compares the \textbf{machine written translation} to one or several \textbf{human written translations} and computes a \textbf{similarity score}
\begin{itemize}[label=-]
	\item Measures the ratio of matched n-grams (overlap) to the total number of n-grams in the translated text (range $[0..1]$ but often scaled to $[0..100]$)
	\item Typically 1,2,3 and 4-gram precision which are equally weighted
	\item Penalty for translations that are too short using a brevity penalty (BP) factor
	\begin{align*}
		\text{BLEU} &= \text{BP}\cdot e^{\sum_{n=1}^{N} w_n\log(p_n)}\\
		p_n &= \frac{\sum_{\text{n-gram}\in \hat{y}} \overbrace{\text{count}_{\text{clip}}(\text{n-gram})}^{\text{\# matched n-grams}}}{\sum_{\text{n-gram}\in \hat{y}} \underbrace{\text{count}(\text{n-gram})}_{\text{\# n-grams in translation}}}\\
		\text{BP} &= e^{\min\left( 0, 1-\frac{\text{len}_{\text{reference}}}{\text{len}_{\text{translation}}} \right)}
	\end{align*}
	\begin{itemize}
		\item[N] n-grams considered up to length N
		\item[$w_n$] n-grams specific weight (default $\dfrac{1}{N}$)
	\end{itemize}
\end{itemize}
BLEU should be used on corpus level, not on individual sentences, and while it is useful, it is imperfect. There are many valid ways to translate a sentence, and a good translation can get a poor BLEU score because it has a low n-gram overlap with the provided human translations.

\subsection{Notes on Machine Translation}
\begin{itemize}
	\item \textbf{Reliability}: NMT translations can be unreliable, offensively wrong, or utterly unintelligible. NMT systems have no guarantees about accuracy and can miss negations, whole words, or entire phrases.
	\item \textbf{Memory}: NMT systems are often built to translate one sentence at a time. As a result, they forget information gained from prior sentences.
	\item \textbf{Common sense}: NMT systems have very little common sense, that is information of the external context and knowledge about the world. Understanding what contexts are appropriate for certain translations is important to our understanding of situations, but these contexts are often difficult to capture in their entirety.
\end{itemize}

\subsection{Attention}
\begin{minipage}{0.65\linewidth}
	The source word encodings $x_1,x_2, \dots, x_T$ are individually weighted depending on the target word $y_{t-1}$. Therefore, there are as many context vectors $c_t$ as there are target words $y_t$ and every context vector $c_t$ for target word $y_t$ is a weighted average of the source words encodings. Depending on the previous target word $y_{t-1}$ the source word encoding will be weighted differently. This method is called global attention\footnotemark.
\end{minipage}
\footnotetext{\href{https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/}{A good example of attention calculated through on a simple Input and Output}}
\begin{minipage}{0.3\linewidth}
\begin{figure}[H]
	\includegraphics[width=\linewidth]{img/seq2seq_attention2}
	\caption{Schematic comparison between a plain seq2seq model (top) and a seq2seq model with attention (bottom)}
	\label{fig:seq2seqattention}
\end{figure}
\end{minipage}

\begin{center}
	\includegraphics[width=0.6\linewidth]{img/seq2seq_attention}
\end{center}

\begin{tabularx}{\linewidth}{m{0.1\linewidth} m{0.3\linewidth} m{0.3\linewidth} m{0.2\linewidth} }
	\textbf{Name} & \textbf{Description} & \textbf{Score} & \textbf{Schema}\\
	\hline
	& & & \\[-1em]
	\textbf{Content-based} & \parencite{graves2014neural} & $\text{score}(s_t,h_i) = \text{cosine}\left[s_t,h_i\right] $ & \includegraphics[width=\linewidth]{img/attention_cosine}\\
	& & & \\[-0.5em]
	\textbf{Additive, Concat} & $\text{W}_a$ is a weight matrix and $v_a $ a weight vector, which both are being trained \parencite{bahdanau2014neural} & $\text{score}(s_t,h_i) = v_a^T\tanh\left(\text{W}_a[s_t;h_i]\right) $ & \includegraphics[width=\linewidth]{img/attention_additive.png}\\
	\textbf{Location-based} & Simplifies the SoftMax alignment to only depend on the target position \parencite{luong2015effective} & $ a_{t,i} = \text{softmax}\left(W_a s_t\right) $ & \includegraphics[width=\linewidth]{img/attention_location.png}\\
	\textbf{Dot-Product} & \parencite{luong2015effective} & $\text{score}(s_t,h_i) = s_t^T h_i $ & \includegraphics[width=\linewidth]{img/attention_dotp.png}\\
	\textbf{Multi\-plicative} & $W_a$ is a trainable weight matrix in the attention layer \parencite{luong2015effective} & $ \text{score}(s_t,h_i) = s_t^T W_a h_i $ & \includegraphics[width=\linewidth]{img/attention_multiplicative.png}\\
	& & & \\[-0.5em]
	\textbf{Scaled Dot-Product} & $N$ is the dimension of the source hidden state. Motivation for scaling is to improve learning when input is large, as the SoftMax may have extremely small gradients \parencite{vaswani2017attention} & $\text{score}(s_t,h_i) = \frac{s_t^T h_i}{\sqrt{n}} $ & \includegraphics[width=\linewidth]{img/attention_scaled_dotp.png}\\
	\caption{Attention score mechanisms}
\end{tabularx}

\noindent
Attention ...
\begin{itemize}
	\item significantly improves NMT performance, the intuition is that it is effective as it allows the decoder to focus on certain parts of the source.
	\item solves the bottleneck problem, as it allows the decoder to bypass the bottleneck and look directly at the source
	\item helps with the vanishing gradient problem, as it provides shortcuts to faraway states
	\item provides some interpretability, by revealing what the decoder is focussing on and by the network learning alignment
\end{itemize}

Attention is a technique to compute a weighted sum of the values, dependent on the query and the keys, which is a selective summary of the information contained in the values, where the query and the keys determine how much to focus on the individual values. Thus, Attention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values), depending on some other representations (the query and the keys).

\section{Understanding Syntax}
Syntactic parsing refers to mapping the syntactic structures of sentences. Formal grammar is a set of rules used to build syntactically valid sentences. Nearly all natural languages seem to follow the Context-Free Grammar model, with Swiss-German being a notable exception needing a mildly context-sensitive grammar\footcite{shieber1985evidence}.

\subsection{Context-Free Grammar}
A context-free grammar includes
\begin{itemize}
	\item a set of terminals $T$\\
	words you see on the page
	\item a set of non-terminals $N$\\
	labels given to words and group of words
	\item a designated start symbol $S\in N$
	\item a set of rules, so-called \emph{productions}, of the form $A\rightarrow\beta$
	\begin{itemize}
		\item $A$ is a non-terminal (Left-Hand Side)
		\item $\beta$ is a sequence of terminals and non-terminals (Right-Hand Side)
		\item $A\rightarrow\beta$ means {\color{red} replace $A$ with $\beta$}
	\end{itemize}
	\item rules can be applied regardless of context (hence context-free grammar)
	\item when a rule is applied to a symbol, nothing matters other than the symbol the rule is applied to
\end{itemize}

\subsection{Key Jargon from Linguistics}
Smaller units combine into increasingly complex ones
\begin{itemize}
	\item phrase, a group of words standing together as a conceptual unit
	\begin{itemize}
		\item Noun Phrase, a phrase built around a noun
		\item Verb Phrase, a phrase built around a verb
		\item Prepositional Phrase, a phrase including a preposition and a noun phrase that indicates relations in space and time
		\item Adjective or Adverb Phrase, a phrase built around adjectives and adverbs
	\end{itemize}
	\item clause, includes at least a subject and a verb
	\item sentence, a group of clauses
	\begin{itemize}
		\item simple sentence, only an independent clause
		\item compound sentence, at least two independent clauses
		\item complex sentence, one  independent clause and one or more dependent clauses
		\item compound-complex sentence, two or more independent clauses and one or more dependent clauses
	\end{itemize}
\end{itemize}

A context-free grammar represents how words and phrases are organised in a sentence. If a sentence is a part of the language defined by a context-free grammar, the organisation of its constituents is known. In practice a probabilistic context-free grammar (PCFG) is needed, where every rule has a certain probability, with the sum over all rules being one.

\subsection{Constituency Structure}
\begin{minipage}{0.6\linewidth}
	\begin{itemize}
		\item \emph{interest} and \emph{rates} go together as noun phrase
		\item \emph{raises} and \emph{interest rates} go together as a verb phrase
		\item \emph{Fed} and \emph{raises interest rates} go together as a sentence
		\item Each NP and VP unit is a \textbf{constituent}
		\item Constituents can be indicated with square brackets\\
		Fed [raises [interest rates]]
	\end{itemize}
\end{minipage}
\hspace{\fill}
\begin{minipage}{0.35\linewidth}
	\begin{forest}
		[S
			[NP
				[NNP
					[\emph{Fed}]
				]
			]
			[VP
				[VBP
					[\emph{raises}]
				]
				[NP
					[NN
						[\emph{interest}]
					]
					[NNS
						[\emph{rates}]
					]
				]
			]
		]
	\end{forest}
\end{minipage}
To determine what a constituent is a complex question and different linguists often disagree on. But there are some basic tests
\begin{itemize}[nosep]
	\item Look for words that stay together when phrases move around
	\item Phrasal expansion and substitution\\
	I sat [on the box]\\
	I sat [on the box | right on top of the box | on top of it | there]
	\item Lots of other tests
\end{itemize}

\subsubsection{Clause-Level Constituents}
\begin{itemize}[leftmargin=*, labelindent=1.5cm, labelsep=0.5cm, noitemsep]
	\item[S] simple declarative clause
	\item[SBAR] clause introduced by a (possibly empty) subordinating conjunction
	\item[SBARQ] direct questions introduced by a wh-word or a wh-phrase
	\item[SINV] Inverted declarative sentence
	\item[SQ] Yes or No questions
\end{itemize}

\subsection{Dependency Parsing}
\begin{minipage}{0.45\linewidth}
	The basic idea behind dependency parsing is that every word is another word’s dependent, except for the root. This root corresponds to the central theme of the sentence.
\end{minipage}
\hspace{\fill}
\begin{minipage}{0.45\linewidth}
	\centering
	\includegraphics[width=0.6\linewidth]{img/dependency_parsing}
\end{minipage}

\subsection{Key Algorithms}
\begin{itemize}
	\item Probabilistic Cocke-Kasami-Younger (CKY) Parser, Constituency Parsing
	\item Arc-Standard Transition-Based Parser, Dependency Parser
	\item Neural Dependency Parser
\end{itemize}

\subsubsection{Arc-Standard Transition-Based Parser}
At any given time, the parser has a configuration
\begin{itemize}[nosep]
	\item \stack of words (left to right)
	\item \buffer of words (left to right)
	\item (partial) \dependencygraph
\end{itemize}

\vspace{1em}
\noindent
The parser begins in an \textbf{initial configuration}
\begin{itemize}[nosep, leftmargin=*, labelindent=3cm, labelsep=0.5cm]
	\item[\stack] ROOT symbol
	\item[\buffer] all words
	\item[\dependencygraph] empty
\end{itemize}

\vspace{1em}
\noindent
The possible actions are
\begin{enumerate}
	\item SHIFT: move a word from the top of the buffer to the top of the \stack
	\item $\text{LA}_r$: Left Arc
	\begin{itemize}[nosep]
		\item the neighbour of the head of the \stack is a dependent of the head of the \stack
		\item add a corresponding link to \dependencygraph with label $r$
		\item remove the neighbour from the \stack
	\end{itemize}
	\item $\text{RA}_r$: Right Arc
	\begin{itemize}[nosep]
		\item the head of the \stack is a dependent of its neighbour
		\item add a corresponding link to \dependencygraph with label $r$
		\item remove the head of the \stack
	\end{itemize}
\end{enumerate}

\subsubsection{Neural Dependency Parser}
Represent words as $d$-dimensional word embeddings with $d\approx 1000$, do the same to represent PoS tags and already known dependency labels. Extract a set of tokens based on stack or buffer positions. Look up their embedding vectors and concatenate them with the PoS and label embedding vectors.
\begin{center}
	\includegraphics[width=0.7\linewidth]{img/neural_dependency_parsing_vector}
\end{center}
\begin{itemize}
	\item Input layer, the concatenated embedding vectors
	\item Feedforward neural network with ReLu non-linearity
	\item Output Layer, Softmax gives a probability distribution
	\item Using Cross-Entropy Loss to measure the error
\end{itemize}

\subsubsection{CKY Constituency Parsing}
CKY uses a parse chart
\begin{figure}[H]
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{img/cky_constituency_parsing_graph01.png}
		\caption{Fill in the cells with the things that can be described in single words}
	\end{subfigure}
	\hspace{\fill}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{img/cky_constituency_parsing_graph02.png}
		\caption{Fill in the second row cells that describe combinations of two words}
	\end{subfigure}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{img/cky_constituency_parsing_graph03.png}
		\caption{Fill in the third row cells that describe three-word constituents}
	\end{subfigure}
	\hspace{\fill}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{img/cky_constituency_parsing_graph04.png}
		\caption{Fill in the fourth row cells that covers the whole span}
	\end{subfigure}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/cky_constituency_parsing_graph_example}
	\caption{In practice it is easier to tilt the chart}
\end{figure}

\section{Information Extraction}
The goal of information extraction is to extract the unstructured information embedded in texts and transform it into structured data. Structured data can be used to populate a relational database to enable further processing or anything else that requires structured input data.

Typical information extraction tasks
\begin{itemize}
	\item \textbf{Named Entity Recognition}\\
	to find names of people, places, organisations
	\item \textbf{Co-reference resolution and entity linking}\\
	to group together named entities into sets corresponding to real-world entities
	\item \textbf{Relation extraction}\\
	to find semantic relationships among entities in the text
	\item \textbf{Event extraction and co-reference}\\
	events are denoted by verbs or nouns, but temporal order matters very much
	\item \textbf{Template filling}
\end{itemize}

\subsection{Named Entity Recognition}
Fine-graded tags are often used
\begin{itemize}[leftmargin=*, labelindent=1.5cm, labelsep=0.5cm]
	\item[GPE] Geo-Political Entity
	\item[FAC] Facility
	\item[NORP] Nationalities, Religious or Political Groups
\end{itemize}

\subsubsection{Named Entity Recognition as Sequence Labelling}
NER is a word-by-word sequence labelling task where the tags capture the boundary and type. Use a sequence classifier to label the tokens with {\color{red} IO}, {\color{Green3} IOB} or {\color{blue} IOBES} tagging
\begin{itemize}[leftmargin=*, labelindent=2cm, labelsep=0.5cm]
	\item[{\color{red} I-TYPE}] inside TYPE entity
	\item[{\color{red} O}] outside any entity
	\item[{\color{Green3} B-TYPE}] beginning of TYPE entity
	\item[{\color{blue} E-TYPE}] end of TYPE entity
	\item[{\color{blue} S-TYPE}] singleton TYPE entity
\end{itemize}
IO, when compared to IOB, has only I entities and no B-TYPEs, this makes things easier for machine learning system but at the cost of more ambiguity.

\subsubsection{Features for NER}
\begin{itemize}
	\item Syntactic chunk labels, which allow parsing through basic constituent analysis
	\item Gazetteer, list of toponyms
	\item Word shapes, which are used to represent the abstract letter pattern of a word by mapping lower-case letters to $x$, upper-case to $X$, numbers to $d$, and retaining punctuation
\end{itemize}

\subsection{Neural Methods}
Two basic intuitions
\begin{itemize}
	\item Names often consist of multiple tokens, so each tagging decision requires joint reasoning over multiple tokens
	\item Token-level evidence that a token represents a name includes orthographic (use character-level embeddings) and distributional evidence (use word embeddings)
\end{itemize}
Feeding the output layer of a RNN directly to a softmax does not allow to impose any constraints on the tag sequence. To impose strong constraints on the output, a Conditional Random Field layer on top of the Bi-LSTM output is used. Bidirectional RNNs are used to capture the left and right context of each word.
\begin{center}
	\includegraphics[width=0.6\linewidth]{img/BiLSTM_CRF}
\end{center}
For a word sequence $\textbf{X}$ and a sequence of predictions $\textbf{y}$, a score is computed
\begin{equation*}
	s(\textbf{X},\textbf{y}) = \sum_{i=0}^{n}A_{y_i,y_{i+1}} + \sum_{i=1}^{n} P_{i,y_i}
\end{equation*}
$A_{y_i,y_{i+1}}$ is the transition score (going from tag $y_i$ to tag $y_{i+1}$), and $P_{i,y_i}$ is the emission score (labelling word $i$ as $y_i$)

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/BiLSTM_CRF_scores}
	\caption{The CRF scores are not a probability but the raw output of the RNNs}
	\label{fig:bilstmcrfscores}
\end{figure}

A softmax layer over all possible tag sequences $\textbf{Y}_{\textbf{X}}$ gives a probability for the sequence of predictions $\textbf{y}$
\begin{equation*}
	p(\textbf{y}|\textbf{X}) = \frac{e^{s(\textbf{X},\textbf{y})}}{\sum_{\tilde{\textbf{y}}\in\textbf{Y}_\textbf{X}} e^{s(\textbf{X},\tilde{\textbf{y}})}}
\end{equation*}
The global normalisation prevents the label bias problem of Maximum Entropy Markov Models.

\subsubsection{Character-Level Embeddings}
Such embeddings are very useful to represent unknown words based on their characters.

\vspace{1em}
\noindent
\begin{minipage}{0.6\linewidth}
	\begin{itemize}
		\item Input: sequence of characters
		\item Projection Layer (PL): lookup table to capture similarities across characters
		\item Forward LSTM: returns a state sequence corresponding to the sequence of character representations from the projection layer
		\item Reverse LSTM: returns a state sequence corresponding to the inverse sequence
		\item Combine the two sequences into an embedding vector
	\end{itemize}
\end{minipage}
\hspace{\fill}
\begin{minipage}{0.35\linewidth}
	\begin{center}
		\includegraphics[width=\linewidth]{img/BiLSTM_character_embedding}
	\end{center}
\end{minipage}

\section{Contextual Embeddings and Transformer Architecture}
Word embeddings have revolutionized the field of Natural Language Processing
\begin{itemize}
	\item 2013: Word2Vec
	\item 2014: GloVe
	\item 2015: FastText
\end{itemize}

But the problem with word representations is that a \textbf{word type has the same word embedding regardless of context}. Words also have different aspects depending on semantics, syntactic behavior, register or connotations. The solution to this is contextual word embeddings.

\subsection{ELMo}
ELMo is based on a bidirectional language model and a Bi-LSTM. Their goal was to make the language model reasonably compact so people can use it even with limited hardware. ELMo has two BiLSTM layers with 4096 units and an output projected to 512 dimensions, with residual connections from the first to second layer.

\begin{center}
	\includegraphics[width=0.15\linewidth]{img/ELMo_input_transformations}
\end{center}
Character embeddings allow to capture morphological features and handle out-of-vocabulary words, while convolutional filters capture local patterns. The key idea behind ELMo was to not just use the output, but use all the available hidden layers and combine them in a task-specific way with task-dependent trainable weights that you learn for each task.

\begin{center}
	\includegraphics[width=0.7\linewidth]{img/ELMo_key_idea}
\end{center}

\subsection{Transformer Architecture}
Quick Overview:
\begin{itemize}
	\item Positional Embeddings: Models relative positioning and ordering of words
	\item Self-Attention: Encodes the context (contextualized word embeddings)\\
	Long-distance context has equal opportunities (without positional encoding it cannot even tell the difference since the path length of all words is one)
	\item Multi-Head Attention: Independently and simultaneously focus on different positions of the input in parallel
	\item Feed-Forward layers: Computes non-linear hierarchical features
	\item Residual Connections: Flow of positional information across layers
	\item Layer-Normalization: Makes it possible to train the network
	\item Transformer Architecture is designed to allow for calculations in matrix form which can be efficiently implemented on GPUs/TPUs
\end{itemize}
Transformers were introduced in the paper \citetitle{vaswani2017attention} by \citeauthor{vaswani2017attention} to address the problem that RNNs require sequential calculations by design, which prohibits parallelisation and thus slows down training. The transformer solves this problem by employing an Encoder/Decoder structure.
\begin{itemize}
	\item \textbf{Encoder}:\\
	N stacked \textbf{multi-head attention layers}, followed by a fully connected feed forward layers
	\item \textbf{Decoder}:\\
	N stacked \textbf{masked} multi-head attention layers, followed by multi-head attention layers (with input from encoder and decoder layers), followed by a fully connected feed forward layers
\end{itemize}
\begin{figure}[htb]
	\begin{subfigure}[t]{0.45\linewidth}
		\centering
		\includegraphics[width=0.8\linewidth]{img/transformer_architecture}
		\caption{Simplified Encoder and Decoder architecture of a Transformer}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\linewidth}
		\centering
		\includegraphics[width=0.8\linewidth]{img/transformer_stacked_enc_dec}
		\caption{Exploded view of stacked encoders and decoders}
	\end{subfigure}
\end{figure}
In the first layer the input words are mapped to their embeddings, the words in each position flow through their own paths, which can be executed in parallel. Self-attention looks at other positions in the input sequence for clues that lead to a better encoding for the current word, which results in contextualised word embeddings.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{img/transformer_context_embedding}
	\caption{Different self-attention embeddings for 'it' depending on position and context}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.4\linewidth]{img/transformer_self_attention_matrix_form}
	\caption{Self-attention calculations done in matrix form for faster processing}
\end{figure}

The attention capacity is increased by splitting attention heads using different projections or subspaces. This improves the performance of the attention layer by providing multiple representation subspaces, with multiple Query, Key and Value weight matrices projecting the inputs (word embeddings or vectors from encoders/decoders) into different subspaces.
Thus expanding the model's ability to independently and simultaneously focus on different positions of the input sequence in parallel.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\linewidth]{img/transformer_attention_head}
	\caption{Three attention heads with their results being concatenated and multiplied by an additional weight matrix}
	\label{fig:transformerattentionhead}
\end{figure}

\subsubsection{Positional Encoding}
Position and order of words are essential parts of any language and are inherently taken into account by RNNs. A transformer however does not have a sense of position or order. This is addressed by adding a vector to each input embedding. These vectors follow a specific pattern that allows to determine the position of each word and the distance between words. This encoding satisfies the following criteria:
\begin{itemize}
	\item Output a unique encoding for each time-step or word position in the sentence
	\item Distance between any two time-steps is consistent across sentences with different lengths
	\item Model generalises to longer sentences without any efforts and with bounded values
	\item Model is deterministic
\end{itemize}
The proposed method by \citeauthor{vaswani2017attention} is to use a $d$ dimensional vector which contains information about a specific position in a sentence. This encoding is not integrated in the model itself but enhances the word with information about its position in a sentence \footnotemark.

\footnotetext{\href{https://kazemnejad.com/blog/transformer_architecture_positional_encoding}{Transformer Architecture: The Positional Encoding by Amirhossein Kazemnejad}}

Let $t$ be the desired position in an input sentence, $\vec{p_t} \in \mathbb{R}^d$ be its corresponding encoding, and d be the encoding dimension (where $d \equiv_2 0$) Then $f:\mathbb{N}\rightarrow\mathbb{R}^d$ will be the function that produces the output vector $\vec{p_t}$ and it is defined as follows:
\begin{align*}
	\vec{p_t}^{(i)} = f(t)^{(i)} & := 
	\begin{cases}
		\sin({\omega_k} \cdot t),  & \text{if}\  i = 2k \\
		\cos({\omega_k} \cdot t),  & \text{if}\  i = 2k + 1
	\end{cases}
\end{align*}
where $\omega_k = \frac{1}{10000^{2k / d}}$. Calculating the correspondent embedding which is fed to the model is as follows
\begin{equation*}
	\psi^\prime(w_t) = \psi(w_t) + \vec{p_t}
\end{equation*}
To make this summation possible, the positional embedding’s dimension has to be equal to the word embeddings’ dimension, thus
\begin{equation*}
	d_{\text{word embedding}} = d_{\text{positional embedding}}
\end{equation*}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.5\linewidth]{img/transformer_attention_residuals}
	\caption{Each sub-layer in each encoder has a residual connection followed by a layer-normalisation step}
	\label{fig:transformerattentionresiduals}
\end{figure}

The decoder has a final linear layer which is  a fully connected neural network that projects the vector produced by the decoder stack into a vector with the size of the output vocabulary. The linear layer is followed by a softmax layer turning the score for each individual word into a probability. The training is done by comparing the ground truth with predicted output for each position and using teacher forcing. This is done by supplying the expected translation instead of predicted output from previous time step, but self-attention layers are only allowed to attend to earlier positions in the output sequence by using masks (see figure \ref{fig:transformerteacherforcing}). Otherwise, the decoder could just attend to the next word in the provided sequence.

\begin{figure}[tbh]
	\begin{subfigure}[t]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/transformer_teacher_forcing}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/transformer_teacher_forcing2}
	\end{subfigure}
	\caption{Masked target output and trained model output}
	\label{fig:transformerteacherforcing}
\end{figure}

\subsection{Transformer Models}

\subsubsection{BERT: Bidirectional Encoder Representation from Transformers}
Pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. Pre-trained BERT representations can be fine-tuned with just one additional output layer to create models for a wide range of tasks without substantial task-specific architecture modifications.

Initial training is done in two steps
\begin{itemize}
	\item Perform an unsupervised pre-training on a large amount of unlabelled data to train a general purpose language understanding model by learning a latent representation of the input text
	\item Reuse pre-trained model and perform an individual supervised fine-tuning on a small amount of labelled data for the downstream tasks
\end{itemize}
This provides a Transfer Learning platform for NLP tasks. BERT uses WordPiece tokenisation for input representation. For Pre-Training Masked Language Modelling (MLM), where $k\%$ of the input words are randomly masked and the model has to predict the original words based on the context, and Next Sentence Prediction (NSP) is used. The goal of NSP is to learn relationships between sentences by predicting if a sentence B is an actual sentence that proceeds sentence A, or just a random sentence.

This pre-trained BERT can be used to create contextualised word embeddings for existing models.

\subsubsection{Reducing the Size of Trained Models}
\begin{itemize}
	\item \textbf{Distillation}\\
	Train a smaller model to replicate the behaviour of the original model
	\item \textbf{Pruning}\\
	Make model smaller by removing not- or less-important Weights or Neurons, Layers, or Heads
	\item \textbf{Quantisation}\\
	Approximate the weights of a NN with lower precision (int8 instead of float32 often works quite well)
	\item \textbf{TensorRT}\\
	Platform for high-performance DL inference
\end{itemize}

\subsubsection{Summary}
\begin{itemize}
	\item Pre-training and fine-tuning Transformer models works very well
	\item Models are extremely expensive
	\item Improvements come from even more expensive models and more data
	\item One option to address the inference or serving problem is through distillation
\end{itemize}

% ------------ APPENDIX ------------ %

\clearpage
\printbibliography

\end{document}
